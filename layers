from matrices import mm
import math

class Layer:
    def __init__(self):
       self._input = None
       self._output = None
       self.dE_dO = None
       self.dE_dI = None
       self.NNindex = None
       
    def forward(self, inp):
        pass
    
    def backward(self, dE_dO, rate):
        pass
    
    def getType(self):
        pass
    
    def setIndex(self, num):
        self.NNindex = num

class Hidden(Layer):
    def __init__(self, neurons):
        self.neurons = neurons
        self.weights = None
        self.NLbiases = None
        self.input_matrix_shape = None
        
    def forward(self, inp):
        if type(inp[0][0]) == list:
            self.input_matrix_shape = mm.dim(inp[0])
            newinp = []
            for matrix in inp:
                for row in matrix:
                    for item in row:
                        newinp.append([item])
            inp = newinp 

        if self.weights == None:
            return inp
        else:
            outp = mm.summatrix(mm.dotprod(self.weights, inp), self.NLbiases)
            self._input = mm.copy(inp)
            #self._output = mm.copy(outp)
            return outp
    
    def backward(self, dE_dO, rate):
        if self.weights == None:
            return dE_dO
        else:
            inp = mm.copy(self._input)
            weights = mm.copy(self.weights)
            dE_dW = mm.dotprod(dE_dO, mm.transpose(inp))
            dE_dI = mm.dotprod(mm.transpose(weights), dE_dO)
            self.weights = mm.submatrix(self.weights, mm.scalematrix(dE_dW, rate))
            self.NLbiases = mm.submatrix(self.NLbiases, mm.scalematrix(dE_dO, rate))
            
            if self.input_matrix_shape == None:
                return dE_dI
            else:
                x, y = self.input_matrix_shape
                outp = []
                temp_matrix = []
                temp_row = []
                for i in range(len(dE_dI)):
                    temp_row.append(dE_dI[i][0])
                    if len(temp_row) == y:
                        temp_matrix.append(temp_row)
                        temp_row = []
                        
                    if len(temp_matrix) == x:
                        outp.append(temp_matrix)
                        temp_matrix = []
                return outp
                        
                        
    def getType(self):
        return "Hidden"
    
    def setWeights(self, newWeights):
        self.weights = newWeights
        
    def getWeights(self):
        return self.weights
        
    def setNLbiases(self, newBiases):
        self.NLbiases = newBiases
        
    def getNLbiases(self):
        return self.NLbiases
        
    def getNeurons(self):
        return self.neurons 
    

class ReLU(Layer):
    def __init__(self):
        # if input is not 3D, will reshape it. Will ensure that it is made 2D again after, as with backpropagation if necessary
        self.is3D = False
        
    def forward(self, inp):
        self.is3D = type(inp[0][0]) == list
        inp = inp if self.is3D else [inp]
        self._input = mm.copy(inp)
        outp = []
        tmp = mm.dim(inp[0])[1]
        for i in inp:
            for row in i:
                for item in range(tmp):
                    row[item] = max(0.01*row[item], row[item])
            outp.append(i)
        #self._output = mm.copy(inp)
        return outp if self.is3D else outp[0]
    
    def backward(self, dE_dO, rate):
        dR = mm.copy(self._input)
        dE_dO = dE_dO if self.is3D else [dE_dO]
        dE_dI = []
        tmp = mm.dim(dR[0])[1]
        for i in range(len(dE_dO)):
            for row in dR[i]:
                for item in range(tmp):
                    if row[item] > 0:
                        row[item] = 1 
                    else:
                        row[item] = 0.01
            dE_dI.append(mm.hadamard(dE_dO[i], dR[i]))
        return dE_dI if self.is3D else dE_dI[0]
    
    def getType(self):
        return "ReLU"

class Softmax(Layer):
    def forward(self, inp):
        sumofexp = sum(map(lambda n: math.exp(n[0]), inp))
        outp = list(map(lambda n: [math.exp(n[0]) / sumofexp], inp))
        self._output = mm.copy(outp)
        return outp
    
    def backward(self, dE_dO, rate):
        #uses formula dE_dI = (M hadamard (I-M(transposed))). dE_dO
        matrix = [i*len(self._output) for i in self._output]
        dE_dI = mm.dotprod(mm.hadamard(matrix, mm.submatrix(mm.identity(len(self._output)), mm.transpose(matrix))), dE_dO)
        return dE_dI
    
    def getType(self):
        return "Softmax"
    
class Convolutional(Layer):
    def __init__(self, input_depth, output_depth, input_size, kernel_size):
        self.input_depth = input_depth
        self.output_depth = output_depth
        self.input_size = input_size
        self.kernel_size = kernel_size
        self.kernels = None
        self.biases = None  
    
    def forward(self, inp):
        self._input = inp
        outp = []
        for i in range(self.output_depth):
            temp = mm.copy(self.biases[i])
            conv = map(lambda x: mm.convolve(x[0], x[1]), zip(inp, self.kernels[i]))
            for item in conv:
                temp = mm.summatrix(temp, item)
            outp.append(temp)
        return outp
    
    def backward(self, dE_dO, rate):
        inp = self._input
        dE_dK = []
        dE_dI = []
        kernels_t = mm.transpose(self.kernels)

        for i in range(self.output_depth):
            dE_dK.append(list(map(lambda x: mm.convolve(x[0], x[1]), zip(inp, [dE_dO[i]]*self.output_depth))))
            
        for j in range(self.input_depth):
            y = list(map(lambda x: mm.convolve(x[0], x[1], "full"), zip(dE_dO, kernels_t[j])))
            temp = y[0]
            for item in y[1:]:
                temp = mm.summatrix(temp, item)
            dE_dI.append(temp)
            
        self.biases = mm.submatrix(self.biases, mm.scalematrix(dE_dO, rate))
        self.kernels = mm.submatrix(self.kernels, mm.scalematrix(dE_dK, rate))
        return dE_dI
    
    def getType(self):
        return "Convolutional"
    
    def getKernels(self):
        return self.kernels
    
    def setKernels(self, new_kernels):
        self.kernels = new_kernels
        
    def getBiases(self):
        return self.biases

    def setBiases(self, new_biases):
        self.biases = new_biases
        
    def getInput_depth(self):
        return self.input_depth
    
    def getOutput_depth(self):
        return self.output_depth
    
    def getInput_size(self):
        return self.input_size
    
    def getKernel_size(self):
        return self.kernel_size
    
class MaxPooling(Layer):
    
    def __init__(self, size, stride):
        self.size = size
        self.stride = stride
        self.max_pos = None
        self.input_size = None

    def forward(self, inp):
        self.input_size = mm.dim(inp[0])
        outp = []
        max_pos = []
        for i in inp:
            pool = mm.pool(i, self.size, self.stride, "max")
            outp.append(pool[0])
            max_pos.append(pool[1])
        self.max_pos = max_pos
        return outp
    
    def backward(self, dE_dO, rate):
        dE_dI = []
        for i in range(len(dE_dO)):
            indices = self.max_pos[i]
            matrix = mm.makematrix(self.input_size[0], self.input_size[1], 0)
            temp = []
            for item in dE_dO[i]:
                temp+=item
            for j in range(len(temp)):
                matrix[indices[j][0]][indices[j][1]] = temp[j]
            dE_dI.append(matrix)
        return dE_dI
    
    def getType(self):
        return "MaxPooling"
    
    def getSize(self):
        return self.size
    
    def getStride(self):
        return self.stride
'''
NOTES: 
- While neither the backpropagation method for the ReLU or Softmax classes utilise the "rate" parameter, it is still passed in during the backpropagation of all layers.
Thus, it still needs to take it in even if it does nothing with it. Can I just change implementation so that only the Hidden layer gets it? Sure. Will I? No need. Easy roundabout.
'''
