from matrices import mm
import math

class Layer:
    def __init__(self):
       self._input = None
       self._output = None
       self.dE_dO = None
       self.dE_dI = None
       self.NNindex = None
       
    def forward(self, inp):
        pass
    
    def backward(self, dE_dO, rate):
        pass
    
    def getType(self):
        pass
    
    def setIndex(self, num):
        self.NNindex = num

class Hidden(Layer):
    def __init__(self, neurons):
        self.neurons = neurons
        self.weights = None
        self.NLbiases = None
        
    def forward(self, inp):
        if self.weights == None:
            return inp
        else:
            outp = mm.summatrix(mm.dotprod(self.weights, inp), self.NLbiases)
            self._input = mm.copy(inp)
            #self._output = mm.copy(outp)
            return outp
    
    def backward(self, dE_dO, rate):
        if self.weights == None:
            return dE_dO
        else:
            inp = mm.copy(self._input)
            weights = mm.copy(self.weights)
            dE_dW = mm.dotprod(dE_dO, mm.transpose(inp))
            dE_dI = mm.dotprod(mm.transpose(weights), dE_dO)
            self.weights = mm.submatrix(self.weights, mm.scalematrix(dE_dW, rate))
            self.NLbiases = mm.submatrix(self.NLbiases, mm.scalematrix(dE_dO, rate))
            return dE_dI

    def getType(self):
        return "Hidden"
    
    def setWeights(self, newWeights):
        self.weights = newWeights
        
    def getWeights(self):
        return self.weights
        
    def setNLbiases(self, newBiases):
        self.NLbiases = newBiases
        
    def getNLbiases(self):
        return self.NLbiases
        
    def getNeurons(self):
        return self.neurons 


class ReLU(Layer):
    def forward(self, inp):
        self._input = mm.copy(inp)
        for row in inp:
            for item in range(mm.dim(inp)[1]):
                row[item] = max(0.01*row[item], row[item])
        #self._output = mm.copy(inp)
        return inp
    
    def backward(self, dE_dO, rate):
        f_prime = mm.copy(self._input)
        for row in f_prime:
            for item in range(mm.dim(f_prime)[1]):
                row[item] = 1 if row[item] > 0 else row[item]*0.01
        dE_dI = mm.hadamard(dE_dO, f_prime)
        return dE_dI
    
    def getType(self):
        return "ReLU"

class Softmax(Layer):
    def forward(self, inp):
        sumofexp = sum(map(lambda n: math.exp(n[0]), inp))
        outp = list(map(lambda n: [math.exp(n[0]) / sumofexp], inp))
        self._output = mm.copy(outp)
        return outp
    
    def backward(self, dE_dO, rate):
        #uses formula derived by the independant code: dE_dI = (M hadamard (I-M(transposed))). dE_dO
        matrix = [i*len(self._output) for i in self._output]
        dE_dI = mm.dotprod(mm.hadamard(matrix, mm.submatrix(mm.identity(len(self._output)), mm.transpose(matrix))), dE_dO)
        return dE_dI
    
    def getType(self):
        return "Softmax"
    
class Convolutional(Layer):
    def __init__(self, input_depth, output_depth, input_size, kernel_size):
        self.input_depth = input_depth
        self.output_depth = output_depth
        self.input_size = input_size
        self.kernel_size = kernel_size
        self.kernels = None
        self.biases = None  
    
    def forward(self, inp):
        self._input = inp
        outp = []
        for i in range(self.output_depth):
            temp = mm.copy(self.biases[i])
            conv = map(lambda x: mm.convolve(x[0], x[1]), zip(inp, self.kernels[i]))
            for item in conv:
                temp = mm.summatrix(temp, item)
            outp.append(temp)
        return outp
    
    def backward(self, dE_dO, rate):
        inp = self._input
        dE_dK = []
        dE_dI = []
        kernels_t = mm.transpose(self.kernels)

        for i in range(self.output_depth):
            dE_dK.append(list(map(lambda x: mm.convolve(x[0], x[1]), zip(inp, [dE_dO[i]]*self.output_depth))))
            
        for j in range(self.input_depth):
            y = list(map(lambda x: mm.convolve(x[0], x[1], "full"), zip(dE_dO, kernels_t[j])))
            temp = y[0]
            for item in y[1:]:
                temp = mm.summatrix(temp, item)
            dE_dI.append(temp)
            
        self.biases = mm.submatrix(self.biases, mm.scalematrix(dE_dO, rate))
        self.kernels = mm.submatrix(self.kernels, mm.scalematrix(dE_dK, rate))
        return dE_dI
    
    def getType(self):
        return "Convolutional"
    
    def getKernels(self):
        return self.kernels
    
    def setKernels(self, new_kernels):
        self.kernels = new_kernels
        
    def getBiases(self):
        return self.biases

    def setBiases(self, new_biases):
        self.biases = new_biases
        
    def getInput_depth(self):
        return self.input_depth
    
    def getOutput_depth(self):
        return self.output_depth
    
    def getInput_size(self):
        return self.input_size
    
    def getKernel_size(self):
        return self.kernel_size
    
class MaxPooling(Layer):
    
    def __init__(self, size, stride):
        self.size = size
        self.stride = stride
        self.max_pos = None
        self.input_size = None

    def forward(self, inp):
        self.input_size = mm.dim(inp[0])
        outp = []
        max_pos = []
        for i in inp:
            pool = mm.pool(i, self.size, self.stride, "max")
            outp.append(pool[0])
            max_pos.append(pool[1])
        self.max_pos = max_pos
        return outp
    
    def backward(self, dE_dO, rate):
        dE_dI = []
        for i in range(len(dE_dO)):
            indices = self.max_pos[i]
            matrix = mm.makematrix(self.input_size[0], self.input_size[1], 0)
            temp = []
            for item in dE_dO[i]:
                temp+=item
            for j in range(len(temp)):
                matrix[indices[j][0]][indices[j][1]] = temp[j]
            dE_dI.append(matrix)
        return dE_dI
    
    def getType(self):
        return "MaxPooling"
    
    def getSize(self):
        return self.size
    
    def getStride(self):
        return self.stride
'''
NOTES: 
- While neither the backpropagation method for the ReLU or Softmax classes utilise the "rate" parameter, it is still passed in during the backpropagation of all layers.
Thus, it still needs to take it in even if it does nothing with it. Can I just change implementation so that only the Hidden layer gets it? Sure. Will I? No need. Easy roundabout.
'''
    


