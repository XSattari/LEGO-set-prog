             \input{localstuff}

\fancyhead[LO]{Waiss Sattari}
\fancyhead[RE]{Lego Set Sort}
\fancyhead[RO, LE]{\thepage}

\hyphenation{
    hy-phe-na-tion
    num-bered
}
\usepackage{graphicx}
\graphicspath{ {./Images/} }
\usepackage{listings}

\raggedbottom
\begin{document}

\thispagestyle{empty}

\begin{center}
    \LARGE\headingfont{\textbf{Lego Set Sort \linebreak The project for my A-level NEA}}
\end{center}

\begin{center}
\vspace{4pt}
\large
    Waiss Sattari  
    
\small
   Woodhouse College 
\end{center}

\tableofcontents

\section{Analysis}
\subsection{Problem background} % talk about lego sets missing pieces etc [1]

LEGOs, according to the Encyclopedia Britannica, are \textit{"plastic building-block toys that rose to massive popularity in the mid-20th century"} \footnote[1]{https://www.britannica.com/topic/LEGO}. These tiny plastic pieces come in a range of shapes and colours, which allow them to be combined to create whatever one can imagine. The company \emph{LEGO} sells these bricks generally as parts of a set, where all the Lego pieces required to create a certain build are boxed together to be purchased and assembled by a consumer; not unlike a jigsaw puzzle.\bigskip

However, also similar to a jigsaw puzzle, pieces from a Lego set can go missing, rendering the final build incomplete. Furthermore, pieces from different sets could become jumbled up, which would require a tedious process of sorting through every single Lego piece, having to remember which pieces your set requires and how many, until you find all the pieces of a particular set. The 
more of these pieces you own, the harder this set-sorting process is.\bigskip

This is the problem I aim to solve. My project intends to aid with sorting LEGO sets, to keep a stored inventory of sets, to recognise and identify a piece for you while you sort (what I will refer to as 'scanning' a LEGO piece), and update the set accordingly. This would provide an efficient system of keeping inventory as well as aiding in sorting, which would allow the user to know what items are missing and how many so that they know what pieces to purchase to complete their set. The task of recognising LEGO pieces to aid with sorting can only be performed with the use of a machine learning model, to be able to differentiate and identify different LEGO pieces. Thus my project will revolve around creating a program that utilises AI to perform the desired task.\bigskip

The end user would of course be LEGO enthusiasts, people who own multiple LEGO sets and face the aforementioned issues of jumbled up sets and missing pieces. One such LEGO enthusiast is a teenager named Jonah. Jonah owns his fair share of LEGO sets, and has agreed to answer some questions for my research into the requirements of my "Lego Set Sort" program.

\subsection{Research}
\paragraph*{End-user aided research\\\\}


Thanks to Jonah's experience and LEGO knowledge, he has been able to provide information which I may find useful for improving my problem solution, and also present some small requirements my solution must have for it to be of any use to someone like him. Following a conversation with Jonah, I have been able to identify some key points for consideration:\bigskip 
\begin{itemize}
    \item Every LEGO piece has an element number, which identifies the shape of the piece, and a colour. LEGO pieces of a specific element and colour have a unique LEGO number which identifies the specific piece. This information on element and colour would be useful in implementation if the solution requires identifying LEGO shape and colour separately. \bigskip
    \item The LEGO number is an identification number and is unique to every LEGO piece, so this number would be great to use within the program to differentiate between every piece. Utilising this information would also be useful as the user may use this number to easily search for and purchase any missing pieces online, so as to complete their set (e.g. through a website like www.toypro.com, where you can search for a piece by LEGO number to purchase.)\bigskip
    \item Since it is usually the case that the multiple sets that are jumbled up together may contain similar pieces, the user should be able to add the same piece to multiple different sets after it has been 'scanned' by the program. \bigskip
    \item There are hundreds of different LEGO colours and tens of thousands of elements. That said, only a select few colours and elements make up the majority of pieces that exist. Using this information and focusing only on these common pieces will simplify implementation immensely. \bigskip
\end{itemize}
This provided information from Jonah does not only make my solution more efficient, but would also make it of actual use to the targeted audience, so taking these points into consideration when making my requirements are paramount.

\paragraph*{Convolutional Neural network research\\\\}% MLM research

As previously mentioned, being able to differentiate between different LEGO pieces in the process of sorting is only possible through use of a machine learning model, which will be the focus of my project. For this, I will need to implement a specific type of neural network, a convolutional neural network (CNN for short), and my neural network will use supervised learning. The following paragraphs encompass some of the research I have done on what these are for the purpose of creating my own CNN. The research I have done spanned over many hours, so I will not go into too much detail and will only provide some key points.\bigskip

Firstly, what is a machine learning? Machine learning works to uncover the underlying relationship within given data. In the case of the project it would be to find the patterns of lines/edges of images of LEGO pieces, to know which pieces have what features so as to differentiate between them. A machine learning model can be considered as a function: It takes in inputs and returns an output, and has multiple parameters (weights and biases) which it may change to alter the outputs it produces. It is not a fixed function but rather one which is 'trained' on historical data so as to best perform the task it was created for. The training can be 'supervised' or unsupervised', but the only one I am concerned with is supervised learning, as it is how I will train my model for my project.\bigskip

In supervised learning, data is fed into our model with a 'label' of the desired output so that the model can learn and tweak its parameters to improve . If we call our machine learning model \(f(x)\), and the desired outputs as \(y\), then the goal is to have our model's output as close to the desired output such that \(f(x) \approx y\) (no models can claim 100 \% accuracy so it is always an approximation). The model learns against the data and it's labels which we feed in, which is why it is called 'supervised' learning. In my case that would require feeding in images of LEGO pieces along with the LEGO number as a label. However, there are many different types of LEGO pieces, and this training process requires thousands of images to train on to be remotely accurate so it would be difficult to take and label all of these images, both expensive and tedious. Luckily, this problem can be overcome through training on virtual data, using images generated from 3D renders of pieces. It can produce more images with less effort than if I were taking images myself and as long as the 3D models are similar enough to physical LEGO pieces, the neural network won't know the difference, meaning it can train on them.\bigskip

A neural network is a subset of machine learning and it's structure is inspired by the human brain, with nodes as neurons connected in layers together to form outputs from inputs. There can be as many layers and nodes in these layers as needed, with an input layer, an output layer, and 'hidden' or 'dense' layers between them which is where the function occurs. When you input data into the network, you 'propagate' forward through the network layer by layer until the end, where an output is produced. Every node between two layers are fully connected, and these connections have a weight assigned to them. To find the value at a particular neuron, you sum the product of all neurons in the previous layer multiplied by the weight, and add the sum to a bias. You then pass this value through an activation function like sigmoid, tanh or ReLU which introduces non-linearity to the network, allowing for a more complex function. There are many neurons and connections in a neural network, so one would imagine all of these computations to be long and strenuous, however the process is simplified as the neurons and the weights / biases can be represented as matrices, thus the relevant computations may be tackled with vector calculations. If we call the matrix of our neurons of the first layer \(X\) of dimensions \(i \cdot 1\), the matrix of neurons of the second layer \(Y\) with dimensions \(j \cdot 1\), the matrix containing all weights between these layers as \(W\) with dimensions \(j \cdot i\), the matrix of biases as \(B\) with dimensions \(j \cdot 1\) and the activation function as \(\sigma\), we can write the equation for forward propagation as \[Y =\sigma(WX + B)\] The weights and biases are the parameters of the function, they dictate the output and thus may be tweaked to improve the output of the neural network.

\begin{center}
\includegraphics[width=16cm]{Images/nodes.png}
\bigskip
\includegraphics[width=16cm]{Images/equation.png}
\end{center}
These images, taken from The Independent Code on YouTube\footnote{\url{https://youtu.be/pauPCy_s0Ok}} show this. From the first image, we can see how the neurons from the first layer are multiplied to their weights, summed, and added to a bias to find the value of a neuron in the second layer. The second image shows how these may be written in vector form. The images do not display an activation function; The Independent Code, who created the diagrams, believes that the activation function could be treated as its own layer, an activation layer, which exists between two dense layers. It essentially works the same, the output of propagating forwards in the first layer undergoes the activation function in the activation layer, and then this result is fed into the second layer as input. The Independent Code chooses to do it this way as he argues that it makes the relevant code for forward and backward propagation a lot easier.\bigskip

Tweaking the weights and biases is done while testing through a process called back-propagation. It is called this because the process starts from the end of the network, and works its way to the start layer by layer while making the changes to the weights and biases. When our neural network takes input and runs it's course, it compares the output with the desired output on the label of the test data. It finds a cost function by finding the squares of the differences of the outputs with the desired output, and the goal is to minimise this cost by changing the weights and biases. This is gradient descent, and involves finding the derivative of the cost function and using this to find other derivatives via chain rule to find equations showing how much the weights and biases of a layer must change by in order to decrease the cost function. I will not go into detail about it, nor the derivation of relevant equations, but the equations are used to tweak the parameters so as to minimise the cost function. When making the changes, we use a variable called the learning rate to determine how drastically we make our changes, which has a value that we decide on. How drastically we change the values may lead to overshooting or undershooting the perfect, desired values of our parameters, and so affects the rate at which our network learns. Thus, picking the right value for the learning rate is key to minimise the time of learning of the neural network.\bigskip

A convolutional neural network is a type of neural network specialised for dealing with images. It is a neural network with an added convolutional layer, and the network may use multiple of these layers. A convolutional layer involves taking the image and running a convolution on it using a 'filter' otherwise called a 'kernel'. The kernel is slid along the pixels of the image, with the values of the pixels in the kernel and the image it overlaps multiplied, summed, added to a bias, and passed through an activation to find the value of the pixel of a new image. Once the kernel has fully slid along the whole surface of the image, a new image is generated. Multiple kernels may be used since different kernels can extract different patterns from the image, and these generated new images may then be pooled (essentially made smaller) e.g. through max pooling, which splits the image into equally sized boxes, and takes the largest value pixel from each of these to construct a smaller image. This process extracts dominant features and reduces dimensionality of the image to make it easier to compute. The smaller images are then flattened into a vector so that they be passed into a normal neural network, and an output is produced in the end using the softmax classification technique (a technique which simply scales outputs into probabilities). The pixel values of the kernels introduce yet another set of parameters which can be tweaked during back-propagation, along with their biases of course.\bigskip

\begin{center}
\includegraphics[width=16cm]{Images/CNNexample.png}
\end{center}
The image features a visual representation of the constituent layers which may make up a CNN, showing how the image size is made smaller going through layer by layer. It also decomposes the process into two main tasks, feature learning where we are extracting relevant patterns and features from the image, and classification where we run these into dense layers to identify what the image is representing.\bigskip

Sources I have used for my full research include:
\begin{itemize}
\item LeetCode machine learning 101- \url{https://leetcode.com/explore/learn/card/machine-learning-101/287/what_is_ml/}
\item A Comprehensive Guide to Convolutional Neural Networks- \url{https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/}
\item 3blue1brown- \url{https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}
\item far1din- \url{https://youtube.com/playlist?list=PL1sQgSTcAaT7MbcLWacjsqoOQvqzMdUWg}
\item The Independent Code- \url{https://youtu.be/pauPCy_s0Ok}
\end{itemize} 

\paragraph*{Existing solutions\\\\}

There are no current solutions to my exact problem of sorting LEGO pieces by set as far as I can find searching online, though there does exist LEGO sorting machines which sort Lego pieces by colour or piece type. They do not feature the functionality of storing and manipulating sets as mine does, but the AI they use in identifying the LEGO piece to sort is the same as what I aim to create for my project, so in that regard it is similar.\bigskip


One of the most popular of these is the "Universal LEGO Sorting Machine" created by a man named Daniel West, which he features in a semi-viral video on YouTube\footnote{"The WORLD'S FIRST Universal LEGO Sorting Machine": \url{https://youtu.be/04JkdHEX3Yk}}. In the video we see the impressive hardware dedicated to the task of moving and physically sorting the bricks, but this is not a focus of my own project. He does not go into detail about the software aspect of his build in this video. However, on his YouTube channel, West has another video uploaded which does, a video dedicated to the AI of his machine\footnote{"LEGO Sorter AI: How Does It Work?": \url{https://youtu.be/-UGl0ZOCgwQ}}. \bigskip 


In this video, we see that Daniel West also utilises a convolutional neural network, and see he also tackles the problem of getting thousands of labeled training data by using 'synthetic data', computer generated images with labels attached. Unfortunately however, West explains that using this synthetic data does not work due to a problem he calls the "Sim-to-Real problem", where the subtle differences in things like lighting, shadow and texture in the generated images- things we may not pick up on ourselves- throws the neural network off completely. He claims that the problem remains unsolved, but that he was able to use a technique published from a paper in 2017 to get around the problem, called Domain Randomisation. The gist of it is that instead of trying to create generated images which match the physical LEGO pieces perfectly, we generate images with a large range of randomised colour, lighting, background and material, and use these images instead to train the neural network. Because of the large variation, the CNN is able to pick up on the underlying relationships of the data better, so as to not be thrown off by subtle differences in texture and lighting anymore- building what West calls "much better quality connections in the brain", so performing better in recognising physical Lego pieces.\bigskip


Daniel West then goes on to explain how he further improves his results by "Fine Tuning", where he takes a small number of real life LEGO images and does a bit more learning with these to essentially fine tune the neural network. He claims that it results in much more accurate predictions even if only a small number of images are used in this step. Learning from Daniel West's solution will greatly improve my own. Utilising domain randomisation in my own machine learning model will be integral in getting it to actually function, as I now know that generated images by themselves do not work. The fine tuning step will also significantly enhance my own results, and overall will lead to a more accurate convolutional neural network.

\subsection{Proposed solution} % my project

My solution features two key aspects. The first is the scanning of LEGO items by image to identify them with use of a CNN, and the second is to allow the user to keep an inventory of their sets within the program. The first aspect may be used as a standalone feature by the user, who might not want to concern themselves with creating an inventory of their sets and only with to use the LEGO identifying capabilities of the neural network. However, the first aspect also stands to act as an extended functionality of the second: that is, to be used for the purpose of adding LEGO pieces which are scanned to a stored set, which may be used for example when a user is physically sorting through sets. Thus, these two aspects are not completely intertwined nor completely independent. \bigskip


 For the AI to interact with the physical world and "see" the piece it is working with, there is need of a camera. It would therefore be effective if the program could make use of a mobile device, as these are widely available (nearly everyone owns a mobile phone) and so would be convenient for the target audience. It also eliminates the need to purchase extra hardware like a webcam/document camera, which makes it inexpensive both for the user and for myself in the development of this project. \bigskip
 
 However, I do not aim to create a mobile application as I have no experience doing so, and the neural network I create may prove too computationally demanding, especially a problem for those with older/cheaper phones. My solution to this is for the phone to only be used for its camera- that is, to have the program run from a PC with the phone connected to it if and when the user wants to use their phone to "scan" a piece. Doing so can be achieved through an IP webcam application downloaded on the phone, and use of the "OpenCV" library in my code to connect to the phone's camera. Alternatively, there can be the option on the program to upload a file of an image of the LEGO, so the user may choose to take photos using any device of their choice before transferring the files to their PC by whatever means, so that they may upload these files in the program to be used to "scan". This added functionality would also allow the user to scan images of LEGO they find online, or images they are sent by friends, without the need to physically own the LEGO themselves.\bigskip

Regarding the second key aspect, my solution will provide the functionality for the end user to input the information of the set(s) they possess including LEGO number of the item, the amount required of the item and the amount that is currently owned. Storing this data will be done using text files. Upon starting the program, all files are read from to initialise the sets in the program, and the text files are updated after any changes are made by the user to a set. For added practicality, I would like my solution to also provide the user with information on things like element number and colour of an item in a set, and for the items within a set to be sorted by its attributes ascending or descending, such as by LEGO number, element number, colour, amount owned, amount required, or amount needed to complete a set. \bigskip

As for the first key aspect, my solution will feature a method to identify LEGO pieces by an image, received by phone or by uploaded file as previously mentioned, and this is where the 'AI' of the solution is utilised. However, there is a glaring issue with this aspect- there are many different LEGO pieces which would appear similar to the neural network, since different LEGOs share the same shape and colour. The neural network would have to be very large and complex to differentiate between both shape and colour, which would greatly increase time training, computational demand of training, and would require a very large labelled dataset- something that would be very difficult to obtain. \bigskip

To combat this issue, I have decided to instead identify a LEGO item's element and colour separately. Every LEGO piece has a unique element and colour combination, therefore a LEGO item and its identifying number can be deduced from knowing the element number of the piece and its colour. Therefore, the element and colour of the piece will be determined with separate neural networks, and these will be used to determine the LEGO number of the item from a hashtable. After identifying the item, the user is then presented with the information- which includes the LEGO number, element number and colour- after which they may choose to add this item to a set(s) at a specified amount. \bigskip

There are a few online datasets for LEGO pieces. For my solution I will use a dataset from Kaggle by Joost Hazelet.\footnote{https://www.kaggle.com/datasets/joosthazelzet/lego-brick-images} I will not be using the entire dataset, rather I will be hand picking certain LEGO elements. As mentioned by Jonah the LEGO enthusiast, the most common elements make up a large proportion of existing pieces, so it is imperative I focus on these. I will determine what elements by finding the overlap of elements in the dataset and the elements that exist in the top 30 most common elements using a website 'Brick Architect'.\footnote{https://brickarchitect.com/most-common-lego-parts/} From these elements I will select 'nice' images, where the angle of the piece is adequate enough to determine the element easily. This is all for the purpose of making differentiation as easy as possible for my neural network, given that the task is complicated and I do not have access to great resources or time.\bigskip

As for the image classification, I will create an algorithm to deduce an RGB value representing the RGB colour of the LEGO piece from an image. The issue is that although LEGO items have discrete colours, images of items are affected by things like glare, shadows, etc. Thus, the RGB values of two LEGOs of the same colour from separate images will differ from each other. To combat this, I will use another neural network dedicated to identifying a LEGO colour by the RGB value, using a dataset created by the 'ICT Institute'.\footnote{\url{https://ictinstitute.nl/legocolor-computer-vision-dataset/}}\bigskip

I have decided to create my solution using python. I picked Python as many websites rank it first place for "Best language for Machine Learning"\footnote{One such of these websites is https://www.springboard.com/blog/data-science/best-language-for-machine-learning/}, but also as it is a language I am proficient in. Furthermore, it is an object oriented programming language, which is a great paradigm to use for a CNN as it features many loosely related types of layers, and also good since my solution will utilise a lot of classes, for things such as sets and LEGO but also for data structures like stacks or the hashtable. \bigskip

My project does come with many limitations. Of course, there exists the time constraint- creating an AI and testing it on data to a satisfactory level of efficiency is a lengthy process, so given the time constraint my machine learning model may not be as accurate as I could make it. Training my CNN to recognise even a single Lego element would require a great load of images of that one piece, so time will also affect the number of different Lego elements my CNN may learn to recognise. Thus, my machine learning model will be limited in the number of Lego pieces it may recognise.Furthermore, testing a neural network is usually done on expensive equipment over the course of days or even weeks. This represents a monetary limitation as I do not have such resources on my PC, and do not wish to waste too much electricity training. This would affect training time and thus accuracy, and again I would have to make compromises with the amount of elements my neural network can recognise. A limitation of the inventory aspect is the efficiency in which the Lego set information is input into the system by the user. It may prove tedious to have to manually input all the information, especially for large Lego sets. I do not wish to bite more than I can chew, so I will have to make do with a simple implementation of the inventory aspect, which would allow me to focus on the neural network aspect.\bigskip

\subsection{Project objectives} % High+Low level requirements: machine learning models, ui, etc
My objectives are split into two categories, the Neural Network aspect and all it encompasses, and the main program aspect- which would include the stored inventory aspect and all other functionalities of the program. These categories are split into their key objectives, which are broken further into smaller sub objectives. All objectives are ranked by their importance. High-level objectives are denoted with 'H', while low-level objectives are denoted with an 'L'. High level objectives take high priority in being implemented and are what I deem integral to the solution, while low level objectives represent added features and functionality which would be nice to include in the program, or features which would only prove to improve the final solution but are not necessary.\bigskip

Neural network aspect:
\begin{enumerate}
    \item H- There exist the Layer classes, a Neural Network class and a class for all the functions used by these two classes.
    \begin{enumerate}
        \item H- The class of functions will comprise of all the functions related to manipulating matrices. This will include creating a matrix of a given size, creating an identity matrix of a given size, matrix addition, subtraction, transposing a matrix, dot product of matrices, hadamard product of matrices, scaling a matrix, raising the matrix to a power, finding the dimensions of a matrix, changing the dimensions, rotating a matrix, and copying a matrix. It will also include the special matrix operations used by the convolutional layer and max-pooling layer, being to perform a convolution of a matrix with another, and to pool a matrix. Other functions in this class includes calculating the mean-squared error and finding the rate of change of the error, both used for back-propagation, and finally a function to shuffle a list of data, which will be used in shuffling training data when training a neural network.
        \item H- The layer classes will all inherit from a base template layer class, with attributes of input, output, rate of change of these, and base methods of forward propagation, backward propagation, and a method to get the type of layer. These methods will all be overridden and implemented differently by the specialised children classes, and these children classes will have their own attributes as well as getters and setters for these. The layers to implement include:
        \begin{enumerate}
            \item H- Hidden layer, aka fully connected layer
            \item H- ReLU activation layer, introduce non-linearity
            \item H- Softmax activation layer, assigning probabilities to output
            \item H- Convolutional layer, for the CNN
            \item H- Max-pooling layer, for the CNN
            \item L- Dropout layer, for reducing over-fitting of data in training
        \end{enumerate}
        \item H- The neural network class will create an object that holds layer objects, and is essentially the neural network being used. The object will be given a name upon initialisation, which could be used to reference the purpose of it, and it will have the methods to add layers to the neural network object. Other functionalities of the class includes:
        \begin{enumerate}
            \item H- The ability to propagate forwards and backwards through all layers.
            \item H- The functionality to train the neural network on given data using the features listed directly above. Trains on every piece of data for a fixed amount of times specified by the epochs.
            \item L- Training is done based on batch size. A forward propagation is run with data for the amount specified by the batch size, the mean of the rate of change of errors is found, and then this is what gets used to perform backwards propagation.
            \item H- The ability to save a neural network object to a file, and to upload a saved neural network from this file. This feature is imperative.
            \item L- Display the layers held in the neural network object.
        \end{enumerate}
    \end{enumerate}
    \bigskip
    
    \item H- Image data is augmented when used to train (domain randomisation)
    \begin{enumerate}
        \item H- Image undergoes geometric augmentations. The augmentation is decided randomly from multiple options, or multiple augmentations may be applied at once. These would include:
        \begin{enumerate}
            \item H- Crop. This crops the image in such a way that the object remains in the image but is off-centred randomly
            \item L- Rotation. This rotates the image by a random angle.
            \item L- Zoom. This zooms into the image to make the object take up more space in it, while still being fully visible in the image.
        \item H- Image undergoes non-geometric augmentations. These include:
        \begin{enumerate}
            \item H- Brightness. How much image is dimmed decided randomly.
            \item L- Contrast. Again, decided randomly.
            \item H- Sharpness. How blurry image is decided randomly.
        \end{enumerate}
        \end{enumerate}
    \end{enumerate}
    \bigskip
    
    \item H- The image and element of a LEGO item can be deduced from an image
    \begin{enumerate}
        \item H- The element is deduced by having the image passed scaled to an appropriate size before being passed into the element CNN.
        \item H- An algorithm finds the RGB value of the LEGO item from an image
        \item H- The RGB value deduced from an image is passed into a NN for determining the colour of the LEGO item.
        \item H- A hashtable data structure exists which takes element and colour as a key, and has the LEGO number as the value. Upon initialisation of the program, a file containing information on all elements, colours, and LEGO numbers is read, which is used to initialise the hashtable.
        \item H- The element and colour are used to find the LEGO number from the hashtable, and all this information is relayed to the user.
    \end{enumerate}
    \bigskip

Main program aspect:
    \item H- There exists classes for Sets, LEGO pieces, and a class 'AllSets' that contains all the sets in the program.
    \begin{enumerate}
        \item H- The 'AllSets' class contains a list set objects, with options to add and remove sets, as well as getters and setters.
        \item H- The Set class contains a list of LEGO objects. The class contains information on the set such as its name, whether the LEGOs in it have a required amount or not, and how they are sorted and displayed in the set. It also contains methods to manipulate these attributes.
        \item H- The LEGO class contains information on the item's LEGO number, which it uses to deduce its attributes for element and colour, done so using another hashtable that instead uses the LEGO number as a key and the element and colour as values. It also has attributes of amount and required amount, and has methods to manipulate these.
        \item H- A folder contains files with information on all sets. In this file is stored the names of all sets in the program, and initialised into an all sets object when the program is run.
        \item H- There also exists files in the folder for every set, with the exact name of the set. These contain information on the sets attributes, and for the LEGO pieces in the set including their amounts. This is also all initialised into objects at the start of the program.
        \item H- Making any changes to sets or items or amounts leads to changing the objects that represent them, which are then used to update the files stored such that changes made are saved.
    \end{enumerate}

    \item H- Sets and the items they contain can be manipulated by the user.
    \begin{enumerate}
        \item H- Sets can be added or deleted, and must have a unique name.
        \item L- Sets can be uploaded from a stored database of existing official LEGO sets.
        \item L- Sets can be duplicated under a different name or backed up to be restored.
        \item H- Within a set, set information and the LEGO items of the set are displayed, including the items LEGO number, element, colour, amount, required amount and a small image of the LEGO item.
        \item H- The set also displays statistics of the set, such as the number of different LEGO items, and the total number of pieces. If the set has a required amount for pieces, then it also displays the total number required and a percentage of how complete the LEGO set is. \item L- Sets can be filtered by element or by colour.
        \item H- Set information can be changed. This includes:
        \begin{enumerate}
            \item L- Changing whether or not the set's pieces have a required amount
            \item L- Changing the set's name
            \item H- Changing how the set sorts its LEGO items. Sorts can happen by LEGO number, by element, by colour, by date added, by amount inset, by amount required, by amount needed to fulfill required and by percentage of required currently owned. This can all be done ascending or descending.
        \end{enumerate}
        \item H- LEGO items within the set can be changed. This includes:
        \begin{enumerate}
            \item H- Changing the amount of the item in the set.
            \item H- Changing the amount required in the set, if it has amount required.
            \item H- Removing an item from the set.
            \item H- Adding an item to the set. This can be done by LEGO number, or directs user to 'scan' a LEGO image.
            \item L- Min/max the amount of all items in the set. Min sets to 0, which is useful if user wishes to begin sorting set again from jumbled up LEGO and store the new amounts in the program's set. Max sets amount to amount required, if there is an amount required.
        \end{enumerate}
        \item H- When making changes to set, user is taken to separate screen. After making their changes, they may save changes or exit, which cancels all changes made.
    \end{enumerate}
    \bigskip

    \item H- Main program uses a stack to store 'states' of the program that the user interacts with. As the program runs, the top "state" is constantly in effect.
    \begin{enumerate}
        \item H- States are represented by functions, and are essentially the screens that the user sees.
        \item H- Upon entering a new state, the function of the state is pushed onto the main program stack.
        \item H- If the user wishes to return to the previous screen, then the current state is pushed off of the stack.
        \item H- Every function takes the main program stack as an argument. The main program stack object is also used to carry around data between the stacks, such as hashtables and the AllSets object.
    \end{enumerate}
\end{enumerate}

\section{Design}
\subsection{Overall system design}

My solution will have the functionality to identify a LEGO item from an image, and to hold a stored inventory of LEGO sets for the user. 


\begin{center}
\includegraphics[width=10cm]{Images/hierarchy-Page-1.drawio2.png}
\end{center}

At the highest level of abstraction, this hierarchy chart demonstrates the subroutines of the main program. It displays the core functionalities my solution requires, decomposed into the two distinct sections of LEGO identification and LEGO storage. \bigskip

Identifying the LEGO requires the image being uploaded, and finds the RGB value of the LEGO deduce the LEGO colour. It then finds the LEGO element, and uses colour and element to get the LEGO number. Finding element and colour makes use of machine learning models. Creating these involved implementation, training, and tweaking of parameters, which took a great deal of time and effort and made up the bulk of the creation of the solution. Thus, the hierarchy chart of the main program represented above is merely the tip of the iceberg of the solution. \bigskip

The LEGO storage aspect performs the task of retrieving stored LEGO data and set data. It deals with the altering of set information, and updating the stored set data to the new altered data. \bigskip

\paragraph*{Main program flowchart\\\\}

The flowchart below illustrates the processes that take place when the main program is run. The main program makes use of a 'main program stack' and 'program states' to provide the interface for the user to utilise the functionalities of the program. The program states essentially represent the different 'screens' presented to the user as the program runs, and are implemented as subroutines. The functionalities of the program state includes displaying text to the user, getting input from the user, performing specific tasks, and flowing to other states, and all functionalities are implemented differently specific to the purpose of the state.

\begin{center}
\includegraphics[width=4.5cm]{Images/flowchart.drawio.png}
\end{center}

Once the program is run, it begins by retrieving stored LEGO data. This data includes a list of all the LEGO colours and LEGO elements that the program can identify, as well as the LEGO numbers of all items that have these specific elements and colours, and all of this data is stored in a file. Retrieving the data produces two arrays, one containing the colour and one the element, and two hashtables. One hashtable allows for the LEGO number of an item to be looked from its colour and element, and another does the reverse, finding the colour and element of a LEGO item from the LEGO number. \bigskip

The program then creates the main program stack, which is an object implemented as a stack data structure but with attributes and getters/setters for these attributes to allow data to be accessed by the program states as the program runs. This data includes the stored LEGO data we just retrieved, thus the object is initialised with the arrays and hashtables. \bigskip

After this, the information of the user's sets is retrieved from the files that store them, which includes a file containing the names of every set, and a separate file for each of these sets. Each set is made into a set object, containing LEGO objects of items in the set, and all sets get added to an AllSets object, which is stored in the main program stack, which gives program states access to all the sets and the items in each set during program execution. \bigskip

The 'Home screen' program state is then pushed onto the main program stack, as the starting state of the main program. Finally, the main program runs an indefinite loop of executing the state at the top of the stack. During the execution of the state, it may push another state to the top of the stack, or pop itself off, which would result in a different state being the current state. Different states and how they link are covered in more detail later. \bigskip

\subsection{Stack and hashtable data structures}

\paragraph*{Main program stack\\\\}

The main program stack is implemented as a stack data structure. This means that it can store items, in a last in first out 'LIFO' basis, and has the methods to push, peek and pop items. \bigskip

The stack contains an array which stores items, and has a top pointer initialised at index 0 of the list. When an item is being pushed onto the stack, the item is added to the array at the position of the pointer, and the pointer is incremented by one. In performing a peek, the item at the index of the pointer minus one- the item below the position of the pointer- is returned. This is only done if the value of the position is greater than zero, otherwise the stack is empty and there is nothing to return. The same is true in performing a pop. If the pointer is larger than 0 the stack is not empty, so the pointer is decremented by 1 and the item at the index of the new pointer is removed, which can be achieved simply by popping the last item from the array.

This is all represented in the diagram below. The stack at the top represents a stack object before any actions are performed on it. Below it is the same stack after performing a push, then a peek, and finally a pop, which returns it back to its original state.

\begin{center}
\includegraphics[width=15cm]{Images/stack2.drawio.png}
\end{center}

Aside from having the functionality of a stack data structure, the main program stack also has attributes which allows it to hold any data required by program states during their execution. This includes data on LEGO colours/elements, hashtables, and the AllSets object which allows access to every set and their constituent items. These attributes have getters and setters to retrieve and change them. The main program stack class is summarised in the class definition below. \bigskip


\begin{table}[h!]
\begin{tabular}{|l|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Object\\ variables\end{tabular} & Type    & Description                                                                                                                                         \\ \hline
Stack                                                      & List    & Holds the items stored in the stack                                                                                                                 \\ \hline
Pointer                                                    & Integer & \begin{tabular}[c]{@{}l@{}}Points at the location above the last item in the stack. \\ Value represents index of this location\end{tabular}         \\ \hline
CurrentImg                                                 & Object  & The image that the program is currently working with                                                                                                \\ \hline
CurrentSet                                                 & Object  & The set that the program is currently working with                                                                                                  \\ \hline
Elements                                                   & List    & Contains all LEGO elements that the program recognises                                                                                              \\ \hline
Colours                                                    & List    & Contains all LEGO colours the the program recognises                                                                                                \\ \hline
ColourElemHashtable                                        & Object  & \begin{tabular}[c]{@{}l@{}}Hashtable that takes a  colour and element, and returns the \\ LEGO number of the item with these qualities\end{tabular} \\ \hline
NumberHashtable                                            & Object  & \begin{tabular}[c]{@{}l@{}}Hashtable that take a LEGO number and returns the colour \\ and element of this item\end{tabular}                        \\ \hline
AllSets                                                    & Object  & Object which contains objects of every set in the program                                                                                           \\ \hline
\end{tabular}
\end{table}
\bigskip

The main program stack object is instantiated with Elements, Colours, ColourElemHashtable and NumberHashtable taken as arguments. The stack is initialised as an empty list, and pointer as the integer 0. CurrentImg, CurrentSet, and AllSets are initialised as None.\bigskip


\begin{table}[h!]
\noindent\makebox[\textwidth]{
\begin{tabular}{|l|l|l|}
\hline
Method                                                            & Input/Output                                                           & Description                                                                                                                                                         \\ \hline
Push                                                              & Input: program state                                                   & \begin{tabular}[c]{@{}l@{}}Pushes program state onto the stack. Added to \\ stack list at index of pointer, and the pointer is \\ incremented by 1\end{tabular}     \\ \hline
Pop                                                               & None                                                                   & \begin{tabular}[c]{@{}l@{}}If pointer larger than 0, pops last item from stack \\ list. and decrements pointer by 1. Otherwise, print \\ "stack empty"\end{tabular} \\ \hline
Peek                                                              & Output: program state                                                  & \begin{tabular}[c]{@{}l@{}}If pointer larger than 0, returns the item from stack \\ list at index of pointer - 1. Otherwise, print \\ "stack empty"\end{tabular}    \\ \hline
GetElements                                                       & Output: elements list                                                  & Returns list of elements                                                                                                                                            \\ \hline
GetColours                                                        & Output: colours list                                                   & Returns list of colours                                                                                                                                             \\ \hline
\begin{tabular}[c]{@{}l@{}}GetColourElem\\ Hashtable\end{tabular} & \begin{tabular}[c]{@{}l@{}}Output: ColourElem\\ Hashtable\end{tabular} & \begin{tabular}[c]{@{}l@{}}Returns hashtable that has LEGO numbers as \\ values\end{tabular}                                                                        \\ \hline
\begin{tabular}[c]{@{}l@{}}GetNumber\\ Hashtable\end{tabular}     & \begin{tabular}[c]{@{}l@{}}Output: Number\\ Hashtable\end{tabular}     & \begin{tabular}[c]{@{}l@{}}Returns hashtable that has colours and elements \\ as its values\end{tabular}                                                            \\ \hline
GetImg                                                            & Output: image                                                          & \begin{tabular}[c]{@{}l@{}}Returns the image that the program is currently \\ working with\end{tabular}                                                             \\ \hline
SetImg                                                            & Input: image                                                           & \begin{tabular}[c]{@{}l@{}}Sets current image of program to the inputted \\ image\end{tabular}                                                                      \\ \hline
GetSet                                                            & Output: set                                                            & \begin{tabular}[c]{@{}l@{}}Returns the set that the program is currently \\ working with\end{tabular}                                                               \\ \hline
SetSet                                                            & Input: set                                                             & Sets current set of program to the inputted set                                                                                                                     \\ \hline
GetAllSets                                                        & Output: AllSets                                                        & Return AllSets object                                                                                                                                               \\ \hline
SetAllSets                                                        & Input: Allsets                                                         & \begin{tabular}[c]{@{}l@{}}Sets the AllSets attribute to the inputted \\ AllSets object\end{tabular}                                                                \\ \hline
\end{tabular}
}
\end{table}
\bigskip

\paragraph*{Hashtable\\\\}

A hashtable is a dictionary which takes a key, performs a hash function on the key to get an index, and checks an array at the position of the index for the value. However, it is nearly always the case that two keys produce the same index, in what is known as a collision. In the event this happens, the hashtable must handle the collision. \bigskip

Items are added to a hashtable as a list containing the key and the value. If two keys produce the same index location, then the hashtable can simply add the key-value pair to the next available location in the table. When it comes to finding an item, the hashtable goes to the index location calculated with the key and compares the key to the key at the location. If it is a match, then it can return the value. If not, then it knows to keep searching contiguous locations to eventually find what it is looking for. If it reaches an empty location, it knows the value was never added so the hashtable doesn't contain it. This is one way to handle collisions. However, this process can result in many computations to eventually find the value or to deem it not in the table. Thus, I implemented collision handling in a different way. \bigskip

My solution instead adds items to a hashtable as a list containing a key, a value, and an empty list- [key, value, []]. The empty list contains the index locations of collided items. When adding an item, if the table already contains an item at the calculated index location, it runs the same process of finding the next available location. However, this time it gets the index value of the new location and adds it to the collided items list at the original index location. Thus, when it comes to finding an item, if the key at the location does not match, then the hashtable checks the locations for each index in the collided items list at this location. It compares its key to the key at each of these locations, and if it finds a match it has found the item it is looking for. If it finds no matches from any of these locations, then it knows the item does not exist in the table. This method of a collided items list pointing to locations of collided items requires less computations than the first method, especially so the more collisions that occur. \bigskip

The hashtable has a fixed size, the value of which can affect the number of collisions that occur. Generally the less collisions the better, as it means retrieving a value requires less computations in searching for it. Another factor that affects the number of collisions is the hash function used, as different functions result in different indexes calculated and ideally the function should produce as little duplicate indexes from different keys as possible. For my hashtable I decided to use a table size of 179 and a hash function of \( (key*3)\) mod \(size\). Modding by size ensures that the calculated index is always in range of the table. I picked this size and function as they have resulted in the least amount of collisions, found through trial and error. \bigskip

The class definition below details the attributes and methods of the hashtable class.\bigskip

\begin{table}[h!]
\begin{tabular}{|l|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Object\\ variables\end{tabular} & Type    & Description                                                            \\ \hline
Size                                                       & Integer & The size of the hashtable, max number of items it can contain          \\ \hline
Contents                                                   & List    & The table itself, a list of fixed size specified by the size attribute \\ \hline
Collisions                                                 & Integer & The number of collisions from adding items to hashtable                \\ \hline
NoOfItems                                                  & Integer & Total number of items that have been added to the hashtable            \\ \hline
\end{tabular}
\end{table}
\bigskip

A hashtable object is instantiated with Size taken as an argument. The contents is initialised as a list of None values, of size Size. Collisions and NoOfItems are both initialised as 0. \bigskip

\begin{table}[H]
\noindent\makebox[\textwidth]{
\begin{tabular}{|l|l|l|}
\hline
Method        & Input/Output                                                                         & Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\ \hline
GetContents   & Output: Contents                                                                     & Returns the list of contents of the hashtable                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \\ \hline
HashKey       & \begin{tabular}[c]{@{}l@{}}Input: key\\ Output: index\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Returns the index calculated from passing key through\\ a function. Index = (key*3) mod size. Mod by size\\ ensures index is in range of table.\end{tabular}                                                                                                                                                                                                                                                                                                                                                              \\ \hline
SetItem       & \begin{tabular}[c]{@{}l@{}}Input: index, key,\\ value\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}If NoOfItems equal to Size, return "table full".\\ Otherwise, increment NoOfItems by 1. If location\\ at index is None,  there is no collision,  so add \\ {[}key, value, {[}{]}{]} to this location. If there is a collision, \\ then increment collisions by 1, and keep checking\\ subsequent index locations until empty location found.\\ Add {[}key, value, {[}{]}{]} here,  get the index of this location\\ and add it to the collided items list of original location.\end{tabular}                              \\ \hline
GetItem       & \begin{tabular}[c]{@{}l@{}}Input: index, key\\ Output: value or\\ False\end{tabular} & \begin{tabular}[c]{@{}l@{}}Check contents at index. If it is None, item not in table.\\ Return False. Otherwise, compare key to key at index.\\ If match, return value from this index. Otherwise, there\\ could have been a collision. Check collided items list at\\ this location. For every index in the list, compare key to\\ the key at this index location. If there is a match, return\\ the value from the location of the matched key. If there\\ are no matches for all index locations, item not in table.\\ Return False.\end{tabular} \\ \hline
GetCollisions & Output: collisions                                                                   & Returns the number of collisions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\ \hline
GetNoOfItems  & Output: NoOfItems                                                                    & Returns the total number of items in the hashtable                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\ \hline
\end{tabular}
}
\end{table}
\bigskip

There exists an aggregation association between the two data structures. The main program stack contains two hashtable objects, but the hashtables may exist separately of the stack.\bigskip

\begin{center}
\includegraphics[width=10cm]{Images/aggregationhashtablestack.drawio.png}
\end{center}

\subsection{AllSets, Set, and Lego classes}

All of the user's set data is stored in objects. An AllSets object stores all the user's Set objects, which have store information on the set as well as a list of Lego objects, where each Lego object stores information on their type and amount in the set. These objects are created at the start of the program using data stored in text files, and altered during the run of the program as the user makes changes to their sets. After every change to a set is made, the updated object is then used as a blueprint to update the text files so that the changes are saved. \bigskip

\begin{center}
\includegraphics[width=15cm]{Images/allsetsetlegocomposition.drawio.png}
\end{center}
\bigskip

The three classes have a composition association, such that a Set object is part of an AllSets object, and a Lego object is part of a Set object, and neither can exist independently of the object that contains them. The class definitions for these classes are displayed below.\bigskip

\textbf{AllSets class:}

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Object\\ variables\end{tabular} & Type & Description                             \\ \hline
Sets                                                       & List & List containing the objects of all sets \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
Method    & Input/Output     & Description                                                                                                \\ \hline
GetSet    & Output: sets     & Returns the list containing all set objects                                                                \\ \hline
AddSet    & Input: set       & Appends the set object to the list of all set objects                                                      \\ \hline
RemoveSet & Input: set       & Removes the inputted set from the list of all set objects                                                  \\ \hline
NoOfSets  & Output: NoOfSets & \begin{tabular}[c]{@{}l@{}}Returns the number of set objects in the list of all set\\ objects\end{tabular} \\ \hline
\end{tabular}
\end{table}
\bigskip

\textbf{Set class:}
\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Object\\ variables\end{tabular} & Type    & Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\ \hline
Name                                                       & String  & Name of the set                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\ \hline
Set                                                        & List    & List containing all Lego objects that make up the set                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\ \hline
Sort                                                       & Integer & \begin{tabular}[c]{@{}l@{}}A number which references how displayed set items\\ are sorted. There are 14 different sorts, so int is 0-13\\ \\ The following shows what numbers reference what sorts:\\ 0 - Date added: Oldest to newest\\ 1 - Date added: Newest to Oldest\\ 2 - Element Number: Low to High\\ 3 - Element Number: High to Low\\ 4 - Colour: A to Z\\ 5 - Colour: Z to A\\ 6 - LEGO Number: Low to High\\ 7 - LEGO Number: High to Low\\ 8 - Amount in set: Low to High\\ 9 - Amount in set: High to Low\\ 10 - Amount Required: Low to High\\ 11 - Amount Required: High to Low\\ 12 - Amount needed to complete: Low to High\\ 13 - Amount needed to complete: High to Low\end{tabular} \\ \hline
HasRequired                                                & Integer & \begin{tabular}[c]{@{}l@{}}A number which references whether or not the set\\ has a required amount for its items. 0 = No, 1= Yes\end{tabular}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
Method         & Input/Output                                                           & Description                                                                                                                                                                            \\ \hline
GetName        & Output: name                                                           & Returns the name of the set                                                                                                                                                            \\ \hline
SetName        & Input: name                                                            & Sets the name of the set to the inputted name                                                                                                                                          \\ \hline
AddLego        & Input: Lego                                                            & \begin{tabular}[c]{@{}l@{}}Appends the Lego object to the set list containing\\ all Lego items of the set\end{tabular}                                                                 \\ \hline
RemoveLego     & Input: Lego                                                            & \begin{tabular}[c]{@{}l@{}}Removes the Lego object from the set list \\ containing all Lego items of the set\end{tabular}                                                              \\ \hline
GetSort        & Output: sort                                                           & Returns the Sort attribute of the set                                                                                                                                                  \\ \hline
SetSort        & Input: sort                                                            & Sets the Sort of the set to the inputted sort                                                                                                                                          \\ \hline
GetSet         & Output: set                                                            & \begin{tabular}[c]{@{}l@{}}Returns the set list containing all Lego items of \\ the set\end{tabular}                                                                                   \\ \hline
GetHasRequired & Output: HasRequired                                                    & Returns the HasRequired attribute of the set                                                                                                                                           \\ \hline
SetHasRequired & Input: HasRequired                                                     & \begin{tabular}[c]{@{}l@{}}Sets the HasRequired of the set to the inputted\\ HasRequired value\end{tabular}                                                                            \\ \hline
GetSortedSet   & Output: SortedSet                                                      & \begin{tabular}[c]{@{}l@{}}Returns the set list containing all Lego items, \\ with the items sorted. The way they are sorted\\ depends on the value of the Sort attribute\end{tabular} \\ \hline
GetSortName    & \begin{tabular}[c]{@{}l@{}}Input: sort\\ Output: SortName\end{tabular} & \begin{tabular}[c]{@{}l@{}}Returns the name of the sort based off of the\\ value of the inputted sort. eg. If 0 input, returns\\ "Date added: Oldest to Newest"\end{tabular}           \\ \hline
\end{tabular}
\end{table}
\bigskip

\textbf{Lego class:}
\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Object\\ variables\end{tabular} & Type    & Description                                                                                                                                                                                                                                                                                      \\ \hline
LegoNumber                                                 & String  & \begin{tabular}[c]{@{}l@{}}A string representing the LEGO number of the item. The integer\\ value of this LEGO number is used in a NumberHashtable, which\\ is passed in as an argument in the instantiation of a Lego object,\\ to get the colour and element of this LEGO number.\end{tabular} \\ \hline
Colour                                                     & String  & \begin{tabular}[c]{@{}l@{}}The colour of the LEGO item, derived from the LEGO number \\ using a NumberHashtable\end{tabular}                                                                                                                                                                     \\ \hline
Element                                                    & String  & \begin{tabular}[c]{@{}l@{}}The element of the LEGO item, derived from the LEGO number\\ using a NumberHashtable\end{tabular}                                                                                                                                                                     \\ \hline
Amount                                                     & Integer & The amount of the LEGO item within the set that contains it                                                                                                                                                                                                                                      \\ \hline
RequiredAmount                                             & Integer & \begin{tabular}[c]{@{}l@{}}The required amount of the LEGO item within the set that \\ contains it\end{tabular}                                                                                                                                                                                  \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
Method                                                         & Input/Output                                                              & Description                                                                                                                                                                                                                               \\ \hline
GetLegoNumber                                                  & Output: LegoNumber                                                        & Returns the LEGO number of the item                                                                                                                                                                                                       \\ \hline
GetColour                                                      & Output: colour                                                            & Returns the colour of the item                                                                                                                                                                                                            \\ \hline
GetElement                                                     & Output: element                                                           & Returns the element of the item                                                                                                                                                                                                           \\ \hline
GetAmount                                                      & Output: amount                                                            & \begin{tabular}[c]{@{}l@{}}Returns the amount of the item in the set\\ that contains it\end{tabular}                                                                                                                                      \\ \hline
SetAmount                                                      & Input: amount                                                             & \begin{tabular}[c]{@{}l@{}}Sets the amount of the item in the set that\\ contains it to the inputted amount\end{tabular}                                                                                                                  \\ \hline
GetRequired                                                    & Output: RequiredAmount                                                    & \begin{tabular}[c]{@{}l@{}}Returns the amount required of an item\\ within the set that contains it\end{tabular}                                                                                                                          \\ \hline
SetRequired                                                    & Input: RequiredAmount                                                     & \begin{tabular}[c]{@{}l@{}}Sets the amount required of an item within\\ the set that contains it to inputted amount\end{tabular}                                                                                                          \\ \hline
\begin{tabular}[c]{@{}l@{}}GetNeededTo\\ Complete\end{tabular} & \begin{tabular}[c]{@{}l@{}}Output: AmountNeeded\\ ToComplete\end{tabular} & \begin{tabular}[c]{@{}l@{}}Calculates how many more of the item is\\ needed to reach the required amount.\\ Returns zero if amount is more than or \\ equal to required amount, otherwise\\ returns required amount - amount\end{tabular} \\ \hline
\end{tabular}
\end{table}
\bigskip

\subsection{Structure of text files}

Text files are used in the main program to store data on LEGO items and on the user's sets. The program recognises 14 different LEGO colours and 10 different LEGO elements. That makes 140 combinations, however not all of the combinations exist as LEGO pieces. There are 121 official existing combinations from these colours and elements, so 121 LEGO numbers the program recognises. Of course, this only barely scratches the surface of the amount of LEGO pieces that exist, but 121 is enough given the scale of the project. \bigskip

A text file named 'information.txt' stores all of the LEGO data. Every line in the file represents a single item. The first 10 lines of the file contain the 10 LEGO elements, and next 14 lines contains the LEGO colour. The next 140 lines contain the LEGO numbers of the combinations of the LEGO elements and colours. It begins with the LEGO number of the first stored element and first stored colour. After this is the LEGO number for the first element, second colour, and then after the first element third colour, etc. This is repeated for the second element, and then the third etc. At lines where there is no LEGO number for the combination of the LEGO element and colour, a '-' is written.

Example of file structure:
\begin{center}
    \textit{
    Elem1\\
    Elem2\\
    Elem3\\
    ...\\
    Elem10\\
    Colour1\\
    Colour2\\
    Colour3\\
    ...\\
    Colour14\\
    LegoNumOfElem1Colour1\\
    LegoNumOfElem1Colour2\\
    LegoNumOfElem1Colour3\\
    ...\\
    LegoNumOfElem1Colour14\\
    LegoNumOfElem2Colour1\\
    LegoNumOfElem2Colour2\\
    LegoNumOfElem2Colour3\\
    ...\\
    LegoNumOfElem2Colour14\\
    ...\\
    LegoNumOfElem10Colour14\\
    }
\end{center}
\bigskip

A directory named 'userinfo' is used to store the text files that contain the user's set data. It contains a file named 'AllUserSets.txt' which is used to store the names of all of the user's sets, and also contains files for each set named after the name of the set e.g. 'MyFirstSet.txt' if the user has a set called MyFirstSet. \bigskip

The AllUserSets file contains the name of every set, separated by commas. For example, if the user had sets named Set1, Set2 and Set3, the file would look like:
\begin{center}
    \textit{Set1,Set2,Set3,}
\end{center}
It must be noted that every set ends with a comma, even the last, so when splitting the contents of the file by "," to get the names of all the sets in a list, it will include an empty string at the end of the list. \bigskip

A set text file contains all of the information relating to that set. This includes how the LEGO items it presents to the user is sorted, whether or not the LEGO items it contains has a 'required amount', and the LEGO items themselves. For each LEGO item, it holds information on the LEGO number of the item, the amount in the set, and the required amount of the item in the set,all seperated by commas. \bigskip

It has a stored required amount even if the set doesn't have a required amount for it's pieces; this data is simply ignored when presenting LEGO information to the user. This is a purposeful feature, so that required amounts are not reset if the user toggles "has required amount" option off then on again, which may happen by accident. The file does not need to contain data on the LEGO item's colour or element, as these can be found using the LEGO number with the NumberHashtable. It also does not need to store the set name since the file is named after the set. \bigskip

For an example set named 'Set1' that has the default sort option of "Date Added: Oldest to Newest", does have a required amount for its pieces, has a LEGO item 300521 at 14 in the set and 21 required, and has an item 300121 at 10 in the set and 5 required, the text file 'Set1.txt' would exist in the 'userinfo' directory that looks like the following:

\begin{center}
    \textit{
    0\\
    1\\
    300521,14,21\\
    300121,10,5\\
    }
\end{center}
\bigskip

\subsection{Retrieving data from text files}

To recap, when the main program is run it begins by retrieving LEGO data to store in two lists and two hashtables, which it stores in the main program stack object as the object's attributes. It then retrieves the set data by storing it as Set and Lego objects in an AllSets object, before storing the AllSets object in the main program stack as an attribute as well. I would like to clarify that the stored LEGO and set data is not stored on the stack itself, but as the main program's attributes. This is to allow the program states that \textit{are} held in the stack to access this data, which it needs so as to make use of the hashtables or alter set data.

\paragraph*{Retrieving LEGO data\\\\}

\bigskip LEGO data is retrieved from the 'information.txt' text file by the subroutine retrieveLegoData. The subroutine begins by opening the file to read from it. It then initialises empty lists for the elements and the colours, as well as hashtable objects of size 179 for the ColourElemHashtable and for the NumberHashtable. It also creates two variables that represent the number of colours and number of elements, with values of 14 and 10 respectively. \bigskip

It then runs a loop with the number of iterations as the number of elements, each iteration reading a line from the text file and stripping the last character off of it- the newline character '$\backslash$n'- to be left with an element number. It then stores this element number in the element list. \bigskip

It repeats this process with another loop that iterates for the number of colours, to read the next 14 lines of the file. With each loop it also reads a line and strips it of the last character. It stores the colour into the colour list. \bigskip

It then runs a nested loop: for every element in the element list, it runs a loop of every colour in the colour list, where it reads another line from the file and strips it of the newline character to get a LEGO number. Thanks to the structure of the text file, this LEGO number corresponds to the element and colour of the two loops, thus for every iteration in the nested loop we have an element, a colour, and the LEGO number of the item that has these two qualities. It uses these three things to create the hashtables, one to be able to look up a LEGO number from a colour and element combination, and another to be able to look up the colour and element from a LEGO number. If the LEGO number from the line it reads, however, is '-' then it knows that there is no LEGO for the colour and element combination, so it skips it. \bigskip

Otherwise, it begins with the ColourElemHashtable. It gets the index of the colour from its position in the colour list, to essentially convert the colour into a number of a value between 0 and 13. It makes this into a string so that it can concatenate it to the end of the element number, which is also a string, and finally converts this whole item into an integer. This number is what is used as the key for the ColourElemHashtable. With the key, it produces an index using the hashtable's hashKey function, and finally adds the key and value to the hashtable using it's setItem method, that takes in the index, key, and value (the LEGO number in this case) as arguments to store the entry. An example: \bigskip

\begin{center}
    \textit{
    Element = '2420', Colour = 'Orange', LEGO number = '6106027'\\
    Orange is the first item of the Colour list, so ColourList.index(Colour) is equal to 0\\
    Concatenating element and colour: '2420' + '0' = '24200'\\
    key = 24200\\
    index = 105 (since hashKey funtion is (key*3) mod size which is (24200*3) mod 179 = 105)\\
    value = LEGO number = '6106027'\\
    With this, an entry is added to the hashtable object using it's method setItem(index, key, item)\\
    }
\end{center}
\bigskip

The subroutine then goes on to create the NumberHashtable. For this hashtable the key is instead the integer value of the LEGO number. An index is calculated using the key by the hash function. The value for the hashtable entry is created by concatenating the element and colour together, both already strings, separated by a comma so as to be easily split again when the value is retrieved from the table. With the index, the key, and the value, it uses the hashtable's setItem method to add the entry into the NumberHashtable. \bigskip

\begin{center}
    \textit{
    Element = '2420', Colour = 'Orange', LEGO number = '6106027'.\\
    key = 6106027\\
    index = 116 (since hashKey funtion is (key*3) mod size which is (6106027*3) mod 179 = 116)\\
    value = Element + ',' + Colour = '2420,Orange'\\
    With this, an entry is added to the hashtable object using it's method setItem(index, key, item)\\
    }
\end{center}
\bigskip

Finally, the subroutine returns the element list, the colour list, the ColourElemHashtable and the NumberHashtable. The main program calls the subroutine and uses all of these returned values to instantiate the main program stack object so that it has the LEGO data as it's attributes.

\paragraph*{Retrieving user set data\\\\}

Retrieving the user's set data is done by the subroutine retrieveSetData. The subroutine has a single parameter so as to take in the main program stack object as an argument. It does this so as to have access to the NumberHashtable which it uses to instantiate Lego objects, which requires it to find the element and colour of a LEGO item from the LEGO number. It also needs the main program stack to add the AllSets object to it at the end. \bigskip

The subroutine begins by opening 'userinfo/AllUserSets.txt' to read from the AllUserSets text file. It gets the names of all sets by reading the first and only line of the file which contains all names separated by commas. The subroutine splits this by ',' to be left with a list allsetData. However, as previously mentioned, set names are added with a comma at the end, so even the last set name ends with a comma, resulting in an empty string being stored at the end of allsetData. This item is removed, and what remains is a list of all of the user's set names. An AllSets object is instantiated.\bigskip

A loop is then run. For every name in the list of set names, a Set object is created. The object is instantiated with the name of the set. Since names are unique and every set has a text file which shares it's name, the subroutine uses the name of the set to open the text file of that set from the userinfo directory (\textit{e.g. for a set with the name MyFirstSet, the subroutine opens 'userinfo/MyFirstSet.txt'}). A setData list is created, and every line in the text file is stripped of the newline character and appended to the setData list. What remains is a list that holds all of the data from the text file, with every line of the file as an item in the list. \bigskip

The first line of a set text file contains the sort, so the Set object sets its sort attribute to the first item of the setData list, converted from string to int. The second line of the file contains the hasRequired, so the Set object sets its hasRequired attribute to the int of the second item in the setData list. The rest of the setData list only contains the LEGO items that make up a set, so a loop is run for every item in the setData list after the second item. \bigskip

For every LEGO item contained in the setData list, a legoData list is created, by stripping the LEGO item from the setData list by ','. As a reminder, items are stored as \textit{'LEGOnum,AmountInSet,AmountRequired'}, so the legoData list for every item looks like \textit{[LEGOnum, AmountInSet, AmountRequired]}. This legoData list is used to create Lego objects. \bigskip

A Lego object is instantiated with the first item of the legoData list (the LEGO number of the item), and with the main program stack's NumberHashtable. The Lego object uses the two to find the colour and element of the item, so that the object has it's attributes of LEGO number, colour and element. The 'amount in set' attribute of the object is set as the int of the second item of legoData (the AmountInSet). The subroutine then checks if the Set object that is to contain the Lego object has a required amount for its pieces, using the Set object's getHasRequired method. If it does, then the Lego object sets it's required amount to the third item of the legoData list (the AmountRequired of the item). \bigskip

Finally, the Lego object is added to the Set object of the set that contains it. This process of creating Lego objects, setting the attributes of the object and then adding the object to the Set object is repeated for every LEGO item in the setData list. When all LEGO items have been added, the Set object is added to the AllSets object, and the process of creating a Set object is repeated for the next set using the next name in allsetData. \bigskip

Once all sets have been created and added to the AllSets object, the AllSets object is set as an attribute to the main program stack so as to give access to sets an the LEGO items contained in sets to the program states.

\subsection{Program states}

'Program states' represent the states of the program, the 'screens' that the user is presented with. It works together with the main program stack to provide an interface to the user, to access the LEGO identifying functionality of the program and to navigate and update the user's set information. \bigskip

These states are implemented as subroutine calls. Every state takes the main program stack object as an argument, to allow it to add other states to the stack, to remove itself from the stack, or to access LEGO/user sets data, and this stack object is the only argument passed into any state. Thus, the object also has attributes for current set and current image, as these are set in some states but need to be accessed in others, so making them attributes of the main program stack allows this data to also be accessed by all states. \bigskip

Encapsulating all this data in the main program stack makes it easier for data to be passed around and accessed than if data were passed around as local variables, and only allows the main program states access to the data which would not be the case if it were implemented as global variables. It also allows for a consistent implementation of the state subroutine calls such that every state requires only one argument, being the main program stack. \bigskip

States are pushed onto the stack as their subroutine calls in the form of a string. When the main program stack is executing the top-most state on the stack as the current state, it simply evaluates the string, which calls the subroutine. During the execution of the subroutine, depending on the user's input, it will push another state onto the stack as a string subroutine call, which will be evaluated and executed by the main program's infinite loop after the current subroutine ends. \bigskip

Use of the stack allows for previous states to be returned to, with the items on the stack representing the path taken to get to a state. Every state has the option to return to the previous state, and two different states may flow to the same state. Thus, to return to the previous state, the current state needs only pop itself from the stack, and the next state on the stack that is now being executed was the state that had transitioned to it. The user can effectively trace back the path taken to a state, by returning from one state to the state that had transitioned to it. \bigskip

The following diagram shows the different states of the program, as well as how they transition to one another. It begins with the 'Home screen' state, the very first state the user is presented with which is pushed onto the main program stack at the start of the program. \bigskip

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=18cm]{Images/progstates.drawio.png}}%
\end{figure}
\bigskip

The arrows in the diagram represent state transitions. Bear in mind that every state can return to the state which transitioned to it, but only to that state, and not to another state which could have transitioned to it- hence why the arrows are not represented as being bi-directional. How a state decides what state to transition to depends on the user's input, the actions the user would like to perform. \bigskip

I have implemented my solution as a text program, run in the terminal. Inputs are therefore taken by the user as characters typed in. Every state accepts 'q' as the input to return, and other characters are used to transition to different states. Every state presents the users with the options they have for what actions they can perform, as well as the expected input to carry out the options. It is important that bad inputs are handled, something the program tackles with selection statements and catch blocks where necessary. \bigskip

For example, most state subroutines use a while True loop to get user input, limiting the user to only acceptable inputs. If the input matches the acceptable inputs, which represent options and are presented to the user, then the action of that input is carried out, and the infinite loop is broken out of. This generally ends the execution of the subroutine and allows for the main program stack to evaluate and execute the next subroutine call, which could be a previous state, a new state, or the same state, depending on the actions carried out. \bigskip

The following paragraphs detail the functions of each of the program states, and how they link to other states. There will be a lot of mention of 'transitioning to another state' and 'returning to the previous state'; I would like to clarify that transitioning to another state involves pushing that state onto the main program stack, and returning to the previous state involves popping the current state off of the main program stack. I would also like for it to be noted that every state presents to the user with what they must input to perform what actions, and that for every option that requires a character to be input, both the lowercase and uppercase of the character is acceptable. Each paragraph includes the name of the state, a basic description of it and what it does, before presenting the acceptable inputs of options and what actions these options perform. \bigskip

\paragraph*{Home screen\\\\}

The Home screen is the very first state the user is presented with. It is the home screen of the program, and gives the user the following options. \bigskip

'q' - Quit the program. This terminates the program, which is otherwise run indefinitely.\bigskip

'1' - Lets the user view their sets. This transitions to the View All Sets state.\bigskip

'2' - Go straight to scanning a LEGO piece, using the program's machine learning models.  This transitions to Upload Image Options state. \bigskip

As mentioned in my proposed solution, storing user set data and identifying LEGO are two aspects of my solution, and are not completely intertwined nor completely independent. The scanning option can be used to add items to a set, but I also want for the option to be usable by someone who does not care for the inventory aspect and simply wishes to use the program for it's LEGO scanning capabilities. Hence, the home screen allows the user to view their sets while also giving them the option to go straight to using the neural network to scan a LEGO item. This is also useful if the user already knows what set(s) they wish to add scanned items to, so they won't have to navigate to a set and go through multiple different states first to begin scanning.

\paragraph*{View All Sets\\\\}

This state presents the user with all of their owned sets. Sets are presented by their name, and are numbered, starting from 1, so that accessing a set can be done simply by inputting the number before the set. This is done for efficiency, as typing out the name of a set to access it would be tedious.\\
If the user does not own any sets, they are presented with the message 'It appears you do not have any sets so far. Consider creating one.', and only have the following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

'n' - Allows the user to create a set. Transitions to Create Set state.\bigskip


If the user \textit{does} own sets, then each set is listed and numbered, starting from 1. e.g. If the user owns a set called MyFirstSet, a set called AllMyLego, and a set called LegoCity then they are presented with
\begin{center}
    \textit{
    1- MyFirstSet\\
    2- AllMyLego\\
    3- LegoCity\\
    }
\end{center}
and must input a 1, 2, or 3 to go t MyFirstSet, AllMyLego, and LegoCity respectively.\\
In the event that the user does have sets, they are given extra options: \bigskip

\noindent
'd' - Delete a set. Transitions to Delete Set state.\bigskip

\textit{\textbf{number}} - Lets the user access the selected set. First, it gets the Set object of the selected set using the inputted number. The Set object is set as the current set attribute of the main program stack. State transitions to the View a Set state.

\paragraph*{Delete Set\\\\}

Deletes a set. Presents the user with the names of all of their sets. The set is deleted by inputting the name of the set. This takes the name rather than a number to ensure the user is selecting the correct set to delete, to essentially make it harder for the user to accidentally delete the wrong set. Even once they do input a set name, they must confirm their decision to delete the set. The user is given the following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

\textit{\textbf{set name}} - Delete the selected set. Gets the Set object of the set using the set name. Before deletion, presents the user with a y/n confirmation to delete the set. If 'n' is input, cancels deletion. Returns user to previous state. Otherwise, if 'y' input, then proceeds with deletion. Removes the set object from the AllSets object stored by the main program stack. Updates the AllSets text file to save this change using the updateAllsetsFile subroutine. Removes the text file of the deleted set from the userinfo directory. Finally, returns user to previous state.

\paragraph*{Create Set\\\\}

Creates a set, with a name inputted by the user. This name must be no longer than 1 character and unique, so that no sets already exist with this name. Gets a list of set names of all sets from AllSets object to verify input name is unique. User is given the following option: \bigskip

\textit{\textbf{set name}} - Create a set. Name must be valid, checks if name is longer than 1 character and is unique. If it is, presents y/n confirmation of creation. If 'n' input, returns user to previous state. If 'y' input, then proceeds. Instantiates a Set object with the input name of the set. Adds object to the AllSets object. Updates the AllSets file to save change with updateAllsetsFile. Creates a text file for the set in the userinfo directory, with the file name the same as the set name. Writes a 0 to the first two lines of the file to save the default values of sort and hasRequired for the set. Finally, returns user to previous state.

\paragraph*{View a Set\\\\}

Allows the user to view a set that they have selected from the View a Set state. First, gets the 'current set' Set object from the main program stack, which represents the selected set. Uses this to get the sorted list of LEGO items using the set's getSortedSet method. User is presented with the set name of the set, the set sort (by the sort name, not sort number) and whether or not the set has a required amount for it's pieces. \bigskip

If the user has no items in the set, the user is presented with the message 'You do not appear to have any LEGOs in this set. Consider adding some.'. Otherwise, for each Lego object in the sorted set, the user is presented with the object's LEGO number, element, colour, and amount in set. If the set has a required amount for pieces, then also presents the required amount of the Lego object and the 'amount needed to complete'. The user is given the following options. \bigskip

'q' - Go back. Returns user to previous state.\bigskip

'n' - Change the name of the set. Transitions to Change Set Name state.\bigskip

's' - Change the sort of the set, the order in which Lego items are displayed. Transitions to Change Set Sort state.\bigskip

'r' - Toggle the hasRequired attribute of the set. Sets the hasRequired attribute of the set to 0 if it is 1 or 1 if it is 0. Updates the set file with the subroutine updateSetFile to save this change, passing in the current set object as the argument.\bigskip

'e' - Edit the Lego items in the set. Transitions to Edit Set items state.\bigskip

'a' - Add a LEGO item to the set. Transitions to the Add by Options state.

\paragraph*{Change Set Name\\\\}

This state allows the user to change the name of the current set. First, gets the current set object. Again, the set's name must be longer than 1 character in length and unique. Gets a list of names for all sets, using AllSets object, which is used to verify input name is unique. User is given following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

\textit{\textbf{valid set name}} - Change the name of the set. Once a valid set name is entered, user is presented with a y/n confirmation to change the name to this. If 'n' input, returns user to previous state. If 'y', then proceeds with change. Program gets current name of Set object, which it uses to delete the text file of the current set from the userinfo directory. Changes the name of the current set object to the new name, and updates the set file of this object using the updateSetFile subroutine, passing in the current set object as an argument. This will create a new text file for the object with it's new name, since opening a text file to write to that doesn't exist creates the file in python. Next, the AllSets file is updated with the updateAllsetsFile subroutine, to save the new name. Finally, the user is returned to the previous state.

\paragraph*{Change Set Sort\\\\}

Changes the sort of the current set, the order in which presented LEGO items are displayed. Gets current set object. Presents every sort option, by having a list of numbers 0 to 13 and the corresponding sort name for these numbers next to them using the current set object's getSortName method. User has the following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

\textit{\textbf{valid number (0-13)}} - Change the sort to the selected number. First, changes the current set's sort attribute to the new number. The set is then updated using subroutine updateSetFile, passing in the current set as an argument. Finally, the user is returned to the previous state.

\paragraph*{Edit Set items\\\\}

Edits the Lego items of the current set. First gets the current set object and the object's sorted set list. If the set has no LEGO items, then a message is presented that they have no items to edit, before returning the user to the previous state. \bigskip

Otherwise, presents LEGO items to the user in the same way as it does in the View a Set state- sorted, with all it's attributes displayed. The user is prompted to enter the LEGO number of an item they would like to edit, and only numbers that match the LEGO numbers of items in the set can be input. User is kept in this state until they quit it, so that they can keep editing the set after making a change to an item. User is given the following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

\textit{\textbf{valid LEGO number}} - Selects the LEGO item that the user wishes to edit by it's inputted number. User is presented with further options on what actions they would like to perform on this item. First subroutine gets the Lego object of this item from the LEGO number. Presents the attributes of this selected LEGO item. Gives the user options of what to do with the selected item. If set doesn't have required amount, only present following options:\\
    'q' - Go back. Returns user to previous state.\\
    '1' - Delete the selected Lego item from the set. Removes the Lego object from the current set object. Updates current set using updateSetFile subroutine, passing current set object in as argument. User is returned to the same state so that they may keep editing the set.\\
    '2' - Edit the amount of the selected item in the set. Runs getAmountAsInput subroutine to get the amount from the user. Sets the Lego object's amount attribute to this new amount, updates the current set with the updateSetFile subroutine, and returns user to the same state.
    
    If the set does have a required amount, then the user is given the extra option: \\
    '3' - Edit required amount of the selected item in the set. Runs getAmountAsInput function to get the amount from the user. Sets the Lego object's required amount attribute to this new amount, updates the current set with the updateSetFile subroutine, and returns the user to the same state.

\paragraph*{Add by Options\\\\}

Asks whether the user would like to enter a new LEGO item to the set by scanning the LEGO item, using the program's machine learning model, or whether they would prefer to add the item manually using the LEGO number of the item. The user is presented with the following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

'1' - Add an item by scanning LEGO. Transitions to Upload Image Options state.\bigskip

'2' - Add an item by inputting LEGO number. Transitions to Add by LEGO Number state.

\paragraph*{Add by LEGO Number\\\\}

State allows user to enter an item to the current set by it's LEGO number. First, gets the current set and a list of the LEGO numbers for all items in the current set. Gets input of a valid LEGO number of the item to add from the user. If the inputted LEGO number exists in the list of LEGO numbers of items in the set, then the item already exists in the set, so cannot add this item.\bigskip

If the item is not in the set already, then uses the LEGO number to look up the colour and element of the item using the NumberHashtable from the main program stack. The LEGO number is converted to an integer to be used as the key for the hashtable, which is used with the hash function of the hashtable to get an index value. If the inputted LEGO number contains non-numerical characters, a catch block catches the error of converting the input to an integer, and the user is prompted to enter a number for their input. The hashtable's getItem method is used with the key and index to get the value, the colour and element of the LEGO item. If the returned value is False, then the item does not exist in the hashtable, meaning that the LEGO number is not recognised by the program. Otherwise, the input value is valid. User is given the following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

\textit{\textbf{valid LEGO number}} - Add LEGO item to the current set by LEGO number. Aforementioned process is used to get a valid LEGO number. Once this number is received, the is prompted to input the amount of the item to add to the set. Amount is gotten using getAmountAsinput subroutine. If the set has a required amount for it's items, a required amount is also gotten from the user using the same subroutine. All of the information of the LEGO item that is to be added to the set is presented to the user, with a y/n confirmation of adding the item. If 'n' input, user is returned to previous state. Otherwise, if 'y' input, proceeds with adding the item to the set. The subroutine addItemToSet is called, which takes in the main program stack, LEGO number, current set, amount, and the required amount as arguments. If no required amount was taken from the user, then a 0 is passed in it's place. User is returned to the previous state.

\paragraph*{Upload Image Options\\\\}

Gets the method by which the user uploads an image to scan. This can be done either by using a camera, using their phone/other device, or by uploading the file of the image. The user is presented with the following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

'1' - Upload by image file. Transitions to Upload Image File state.\bigskip

'2' - Upload by use of camera. Transitions to Use Camera Options state.

\paragraph*{Upload Image File\\\\}

Allows user to upload an image file to be scanned. Prompts user to input the full path of an image file. Attempts to open it using PIL using a catch block if it cannot. The image from the file is used to be scanned. User is given the following options in this state: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

\textit{\textbf{valid file path}} - Scan the image from the inputted file. The current image attribute of the main program stack is set to the image from the file opened with PIL. State transitions to the Validation + add to set state.

\paragraph*{Use Camera Option\\\\}

State gives the user options to help, set up and use their device camera to scan. Scanning using the phone requires an IP camera app to be installed on the device, such as IP Webcam. The device can stream through this application, and using the IPv4 provided by the app, the PC can connect to the stream- given that both devices are connected to the internet. Thus, using the app to stream and using the OpenCV library in my program code, it is possible to get the phone to stream data to the PC, which can effectively be used to get the image to scan LEGO. The only requirement is that the program needs the IPv4 of the streaming device to connect to it. Thus, the user is made to set their IPv4 with details on how to do so, and this is stored on the PC so that it only needs to be set once, though the user has the option to change it as they wish, if they wish to stream from a different device or for some other reason. \bigskip

A text file called 'IPv4.txt' is stored in the userinfo directory, and this contains the user's IPv4. The IPv4 is read from the file. The user is presented their IPv4, or 'None' if they have not yet input an IPv4. The user is prompted to set up their camera before attempting to use the feature, and that they have a correct IPv4 input in the program. The user is then presented with the following options:\bigskip

'q' - Go back. Returns user to previous state.\bigskip

'1' - Get help with how to set up and use the camera to scan LEGO. Transitions to Help Using Camera state.\bigskip

'2' - Set or update the stored IPv4. Transitions to Update IPv4 state.\bigskip

'3' - Begin scanning with the camera. If the user has not input an IPv4 yet, then they are prompted to first do so. Otherwise, transitions to Use Camera state.

\paragraph*{Help Using Camera\\\\}

This state simply provides the user with information on how to set up and use their camera. This includes information on downloading an IP camera app, how to get their IPv4, how to begin streaming, inputting IPv4 into program, how to use the phone camera to get an image in the program, how to get the best results out of the images they capture with their phone, etc. The user is presented with the following option: \bigskip

\textit{\textbf{any input}} - Go back. Returns the user to the previous state

\paragraph*{Update IPv4\\\\}

Updates the user's IPv4 which is stored in the program. Reads current IPv4 from IPv4.txt text file in userinfo directory. Displays the user's current IPv4, or 'None' if the file is empty. Prompts user to enter their new IPv4. User is presented with the following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

\textit{\textbf{new IPv4}} - Updates the stored IPv4 to the new value. Opens the IPv4 text file, overwrites the contents of the file to the new IPv4. Finally, returns the user to the previous state.

\paragraph*{Use Camera\\\\}

Uses the phone camera to get an image of a LEGO piece. Begins by retrieving the user's IPv4 from the text file. Must be in form 'https://\textit{IPv4}/video'. If the retrieved IPv4 does not begin with 'https://', then this gets concatenated to the front. Similarly, if it does not end with '/video', this gets added on to the end. \bigskip

The user is then prompted to ensure their app is open and streaming, and is given basic information to help them use the camera. When the user is to proceed, a window will open on their PC showing what the phone camera sees. If they do not see this window, then it has opened in the background. The user must ensure that the window is open and focused on. Then, they must angle their camera to a good angle to capture the best image of their LEGO, and when they are ready to capture the image they must press the space bar on the keyboard. Otherwise, pressing the letter 'q' on the keyboard terminated the scan. The user is presented with the following options: \bigskip

'q' - Go back. Returns user to previous state.\bigskip

\textit{\textbf{any input}} - Proceed with using camera. Program connects to phone and captures it's stream. If 'q' is pressed during connection, connection is broken. User is returned to the same state. If the spacebar is pressed, then a temporary image is taken from the phone's camera. This image is converted to a PIL image, and is set as the current image on the main program stack. State transitions to Validation + add to set state.

\paragraph*{Validation + add to set\\\\}

This state scans the current image using the program's neural networks to identify the LEGO item. It asks the user to validate if the scan is correct, and if so, then gives the user the option to add the item to a set. Begins by getting the current image, as well as the colour list, element list, and ColourElemHashtable from the main program stack. Gets the LEGO number, colour, and element of the LEGO from the image by passing the image, colour list, elem list and ColourElemHashtable into the function getInformation. This function is what uses the neural networks to identify the colour and element from the image, and uses the identified colour and element with the ColourElemHashtable to get the LEGO number.\bigskip

If the LEGO number is False, then the identified element and colour combination does not exist, meaning that one of them was identified incorrectly. In this case, the program apologises and prompts the user to use a better image. Otherwise, the colour, element, and LEGO number are presented to the user, and the user is asked if this information is correct, y/n. If 'n' input, again apologises and prompts for better image. \bigskip

If 'y' input, user given option to add the item to a set, again y/n. If 'n' input, user returned to previous state. If 'y' input, prepares to add item to a set. Gets the AllSets object from the main program stack, and presents to the user the names of all their sets. Asks the user to input the name of the set they would like to add the item to. If 'q' input, returns user to previous state. Otherwise, if input is in list of names of all sets, then input is a valid set name, so proceeds. \bigskip

The Set object with this name is retrieved from the AllSets object, and the LEGO numbers of every LEGO item in the set is stored in a list. If the LEGO number of the item to be added exists in this list, then the set already has the identified item, so the item cannot be added to this set. User is returned to the set selection so they may choose a different set to add their item to. If the LEGO number does not exist in this list already, however, then it can be added to this set, so proceeds. \bigskip

The getAmountAsInput subroutine is run to get the amount of this item to be added to the set. If the selected set has a required amount for it's items, then this required amount is also gotten from the user with the getAmountAsInput subroutine. Finally, the information of the item to be added to the set is presented to the user, along with a message stating that it has been added. The subroutine addItemToSet is called, passing in the main program stack, LEGO number, selected set, amount, and required amount. If no required amount was acquired from the user, a 0 in passed in its stead. The user is returned to the set selection so that they may add the same identified LEGO item to another set if they wish. \bigskip

The following flowchart attempts to explain the decision making more clearly. \bigskip

\begin{center}
\includegraphics[width=12cm]{Images/AddToSetFlowchart.drawio.png}
\end{center}

\subsection{Subroutines}

This section details the subroutines used in the main program. Note that this does not contain the functions used by the neural network. A couple of the functions in this section do utilise neural networks, but for now how the neural network functions will be abstracted away. The neural network will be represented as a black-box function that simply takes in an input and returns an output. The neural network, and the functions that it is built upon will be covered in a later section. \bigskip

The subroutines can be split into procedures and functions. The procedures used in the main program are used for updating and retrieving data stored in text files. The functions used in the main program are used for the purpose of identifying a LEGO item. The following tables provide an overview of the different subroutines as well as a brief description of their purpose. \bigskip

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
Procedure         & Input                                                                                                             & Purpose                                                                                                                                                                                                                 \\ \hline
retrieveLegoData  & none                                                                                                              & \begin{tabular}[c]{@{}l@{}}Retrieves LEGO data from text files, which\\ includes an element list, a colour list, a \\ ColourElemHashtable and ElemHashtable\end{tabular}                                                \\ \hline
retrieveSetData   & Main program stack                                                                                                & \begin{tabular}[c]{@{}l@{}}Retrieves stored data from text files on user\\ sets. Stores this data in Lego and Set objects\\ which are stored in an AllSets object that is\\ held in the main program stack\end{tabular} \\ \hline
updateAllsetsFile & AllSets object                                                                                                    & Updates the Allsets text file                                                                                                                                                                                           \\ \hline
updateSetFile     & Set object                                                                                                        & Updates the text file of the Set object                                                                                                                                                                                 \\ \hline
addItemToSet      & \begin{tabular}[c]{@{}l@{}}Main program stack, \\ LEGO number, Set object,\\ amount, required amount\end{tabular} & \begin{tabular}[c]{@{}l@{}}Creates a Lego object which gets added to a\\ Set object. Then runs updateSetFile to save \\ this change to the set's text file\end{tabular}                                                 \\ \hline
getAmountAsInput  & none                                                                                                              & \begin{tabular}[c]{@{}l@{}}Used to get a valid amount input from the\\ user\end{tabular}                                                                                                                                \\ \hline
\end{tabular}
\end{table}
\bigskip

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
Function       & Input/Output                                                                                                                                       & Purpose                                                                                                                                                               \\ \hline
getInformation & \begin{tabular}[c]{@{}l@{}}Input: Image, element list,\\ colour list, \\ ColourElemHashtable\\ Output: element, colour,\\ LEGO number\end{tabular} & \begin{tabular}[c]{@{}l@{}}Returns all the information of a LEGO item\\ from an image. Does so by using the\\ getColour, getElem and getNumber functions\end{tabular} \\ \hline
getMaxIndex    & \begin{tabular}[c]{@{}l@{}}Input: list\\ Output: index\end{tabular}                                                                                & Returns the index of the largest number in a list.                                                                                                                      \\ \hline
getRGB         & \begin{tabular}[c]{@{}l@{}}Input: image\\ Output: rgb\end{tabular}                                                                                 & \begin{tabular}[c]{@{}l@{}}Returns an RGB value for a LEGO item from\\ an image\end{tabular}                                                                          \\ \hline
getColour      & \begin{tabular}[c]{@{}l@{}}Input: image, colour list\\ Output: colour\end{tabular}                                                                 & \begin{tabular}[c]{@{}l@{}}Returns the colour of a LEGO item from an\\ image, using getRGB and getMaxIndex and a\\ colour neural network\end{tabular}                 \\ \hline
getElem        & \begin{tabular}[c]{@{}l@{}}Input: image, element list\\ Output: element\end{tabular}                                                               & \begin{tabular}[c]{@{}l@{}}Returns the element of a LEGO item from an\\ image, using getMaxIndex and an element\\ neural network\end{tabular}                         \\ \hline
getNumber      & \begin{tabular}[c]{@{}l@{}}Input: element, colour,\\ colour list, \\ ColourElemHashtable\\ Output: LEGO number\end{tabular}                        & \begin{tabular}[c]{@{}l@{}}Returns the LEGO number of a LEGO item\\ from it's colour and element, by using a \\ ColourElemHashtable\end{tabular}                      \\ \hline
\end{tabular}
\end{table}
\bigskip

The following paragraphs detail how each subroutine works. The procedures retrieveLegoData and retrieveSetData have already been explained in detail in the \textit{Retrieving data from text files} section, so these will be omitted.

\paragraph*{updateAllsetsFile\\\\}

This procedure updates the Allsets text file. It does so by overwriting the file to store the necessary information of the changed Allsets object. Thus, it takes in the Allsets object as input. \bigskip

The Allsets text file is opened, and a list of all sets is gotten from the AllSets object using it's getSet method. For every set in this list, the name of the set is written to the file, followed by a comma, in the fashion of how the Allsets text file is structured. 

\paragraph*{updateSetFile\\\\}

This procedure updates the text file of a Set object. It does so by overwriting the file of the object to store the necessary information of the Set object, which has had changes made to it. Thus, it takes in the Set object as input. \bigskip

The name of the Set object is gotten, which is used to open the text file of that set. The sort attribute of the set is gotten and written to the first line. The hasRequired attribute of the set is gotten and written to the second line. The list of all Lego objects in the set is gotten using the set's getSet method. For each Lego object, the LEGO number, amount, and required amount of the object are written to a line in the file, separated by a comma. This is all written in line with the structure of the set text file.

\paragraph*{addItemToSet\\\\}

This procedure creates a Lego object and adds it to a set. It takes in a LEGO number, amount, and required amount to make the Lego object, and a Set object for the set to add the Lego to. It also takes in the main program stack. \bigskip

It gets the ColourElemHashtable from the main program stack, and uses this and the LEGO number to instantiate a Lego object. It then sets the attributes of the object to the amount and required amount, adds the object to the Set object, and then updates the set's text file with the updateSetFile procedure.

\paragraph*{getAmountAsInput\\\\}

This procedure is used to get a valid amount input from the user. Getting an amount from the user is something which happens a lot in the program, hence why a dedicated subroutine exists for this operation. An infinite loop is run, asking the user for an amount to be input. A catch block evaluates whether the int of the input is larger than or equal to 0. If the input is not numerical characters, converting to an integer results in an error which is caught by the block and prompts the user to input a number. If the number is not >= 0, then the user is prompted to enter a positive number. Otherwise, the input is valid and the amount is returned.

\paragraph*{getInformation\\\\}

This function returns all of the information of a LEGO item from an image of the item, done by using the other functions for getting the colour, element, and LEGO number. It essentially combines them within a single function to provide a single interface for identifying a LEGO item from an image. It therefore takes in the inputs that the three functions require to pass them in -Image, element list, colour list and the ColourElemHashtable- and returns the returned values from the three functions, being the colour, element and LEGO number.

\paragraph*{getMaxIndex\\\\}

This function is used to get the index of the largest number in a list. For example, for the list 

\begin{center}
    \textit{
    [0, 4, 3, 74, 0.6, 4, 1]
    }
\end{center}

the function would return the value 3, since index 3 of the list contains the largest number in the list, 74.\bigskip

This function is necessary as the neural network outputs a probability distribution, a list of numbers where each item in the list represents a class (something the input can be classified as being e.g. Orange if the neural network's task is to identify colour), and the value of the item represents the probability of that item being the correct guess. The getMaxIndex function is used to get the index of the class with the highest probability of being correct, to essentially get the neural network's guess for what the object is.

\paragraph*{getRGB\\\\}

This function is used to return an RGB value to represent the colour of the LEGO item from an image. The RGB value is needed to identify the colour of the object, so that it may be input into a neural network trained to identify LEGO colour by RGB values. The reason a neural network is needed for this task is because although the LEGO colours themselves have discrete RGB values, an image of the LEGO does not. It is affected by things such as lighting and shadows, and so contains a range of RGB values, a majority of which may deviate from the LEGO colour's RGB value significantly. Thus, using a dataset I found online of RGB values of LEGOs from images, I trained a neural network to be able to classify LEGO colours using an RGB value of an object from an image. \bigskip

I designed the function to achieve the task of getting an RGB value to represent the item by separating out the image's pixels of the object from the background, using the fact that they are different colours, and then finding the average of every RGB value of the object's pixels to get a single RGB value to represent the object. The function takes in an image as input, and returns the deduced RGB value. \bigskip

All image handling in my project is done using the Pillow library (PIL). I have also designed a class 'Augment' of functions, built on PIL functions, for the purpose of augmenting images in my program. This class of functions is covered in more detail in a later section. One such of these functions is the \textit{aspect} function, which crops an image to be of 1:1 aspect ratio. \bigskip

The getRGB function uses this aspect function to turn the image that it takes in as an argument to be of 1:1 aspect ratio. It then resizes the now square image to a new size, of 32 by 32, using the PIL thumbnail function. A separate grey-scale version of this resized image is created with the PIL ImageEnhance, and both of these images are turned into lists of pixel values with the PIL image's getdata function. \bigskip

Different colours result in a different shade in grey-scale, and grey-scale pixels have RGB values of equal red, green and blue e.g. 138,138,138, so may be considered by a single value, 138. Thus, using a grey-scale version of the image makes it easy to differentiate between the object and background, which are likely to have a significant difference in the RGB value of the grey-scaled image. A value can be found for the background e.g. 240, and for every pixel in the grey-scale image, if the value of the pixel is in a range of about 50 of the background value, then it can be considered part of the background. Otherwise, it is likely to be a different colour, thus can be considered part of the image, and the position of these pixels can be used with the coloured image to get the pixels of the object in the coloured image. \bigskip

The first step is to identify the background colour of the image. This can be achieved by taking the RGB values of the corner pixels of the image, as the object is most likely to be in the center of an image. It is possible that the object touches a corner, but very unlikely to touch more than one. If it does, the object may not even be recognisable in the image to identify it so it is a bad image anyways. The function thus determines the background colour by getting the majority of the corner pixels. \bigskip

The grey scale image list of pixel values is changed so that every pixel in it is represented by a single number and not a tuple for the RGB value, since the red, green and blue are all of the same value anyways. The corner pixels can be found in index positions 0, size-1, size*(size-1) and (size*size)-1 in the list of pixel values. The corner pixels from the grey scaled image are put in a list together, and the maximum value is found. \bigskip

For every number in the corner pixels list, if the number is not within a range of 50 of this max value then the numbers represent different colours and a count is incremented by 1. If by the end the count is 0, then all pixels are of the same colour. If it is 1, then a single different colour was found implying the object touches a corner in the image. The max value still represents the majority colour, so it is assumed the background colour. If the count is 2, then the object likely touches two corners in the image, and in this scenario the max value is randomly assumed to be the background colour. If the count is 3, it is less likely that the object touches three corners and more likely that the max value is the pixel colour of the object and the other 3 corner pixels represent the background, so this is what is assumed. A boolean maxIsBG is created and is assigned True if the maximum value of the corner pixels is assumed to be the background colour, and False if it is assumed to not be the background colour, that the max value pixel is that of the object. \bigskip

Now that we have identified the grey-scale value of the background, we can begin to separate out the object pixels. An empty list is created to hold all of the object's pixels. For an index in range 0 to the length of the image list -1, The pixel value at this index in the grey-scale list is compared to the max value pixel of the corner pixels from earlier. If the pixel value is within a range of 50 of the max value, then they are the same colour. If maxIsBG is True, this means the pixel is of the background. If maxIsBG is False, then it is instead of the object. On the other hand, if the pixel value is not within 50 of the max value pixel, then they are different colours. Therefore if maxIsBG is True then this pixel is of the object, and if maxIsBG is False then this pixel is of the background. In the event that the grey-scale pixel is identified as being of the object, then the pixel in the same position (same index) in the colour list is found, and added to the object pixels list. \bigskip

This leaves us with a list of all the RGB values of pixels that make up the object from the image. Every RGB value in the list is summed together, divided by the number of RGB values, and rounded to the nearest integer to effectively find the average RGB value for every RGB value in the list. This average RGB value of the object is what is returned to the user. \bigskip

The following example intends to make this process clear using an image at a much smaller scale.

\begin{center}
\includegraphics[width=5cm]{Images/getrgbexample.drawio.png}
\end{center}

An 5x3 image containing a blue pixel is input into the function. It changes the ratio to 1x1 by cropping it, and resizes it. In this case we will take the new size to be 3x3, which it already is, so nothing happens. We get the list of the coloured image, and the list of single values for the grey scale image: 
\begin{center}
    \textit{
    Colour list: [(255, 255, 255), (255, 255, 255), ..., (30, 160, 220), (51, 153, 255), (255, 255, 255)]\\
    Grey scale list: [255, 255, ..., 255, 128, 134]
    }
\end{center}
The corner pixels of the grey scale image is found from the grey scale list at indexes 0, 3-1 = 2, 3*(3-1) = 6 and (3*3)-1 = 8. The values are added to a new list

\begin{center}
    \textit{
    Corner pixel list: [255, 255, 255, 134]
    }
\end{center}
The max of the list is 255. The first three items are within 50 of this max value, but the last item is not (255 - 134 = 121, 121 > 50) so the count is 1. This implies that since the max value has majority, it represents the background colour. Thus maxIsBG = True.

For every item in the grey scale list, indexes 4, 5, 7 and 8 contain values which are not within 50 of the max value, 255. Therefore, since maxIsBG is True, they are not of the background and instead represent pixels of the object. The items in the colour list at these indexes are added to an object pixels list. 

\begin{center}
    \textit{
    Object pixels list: [(50, 150, 255), (52, 149, 240), (30, 160, 220), (51, 153, 255)]
    }
\end{center}
The average RGB value from this list is calculated: 
\begin{center}
    \textit{
    sum = [(50+52+30+51), (150+149+160+153), (255+240+220+255)] = [183, 612, 970]\\
    Average = sum/total = [(183/4), (612/4), (970/4)] = [47.75, 153.0, 242.5]\\
    RGB = average rounded = [48, 153, 243]\\
    }
\end{center}
So the RGB value deduced from the object is (48, 153, 243). This tuple is returned to the user.

\paragraph*{getColour\\\\}

This function is used to return the colour of a LEGO item from an image of the item. It takes in the image as an input, as well as the colour list. \bigskip

It begins by passing the image into the function getRGB to get an RGB value of the item in the image. It then normalises this value so that every item is less than 1, done by dividing the numbers in the RGB value by 255, and reshapes it to a 3x1 matrix. The function uploads the saved Colour Neural Network, the neural network trained to identify colour. The normalised RGB matrix is input into the neural network, and the output is a 14x1 matrix of the probabilities of the RGB value being from the 14 different colours. This matrix is turned into a list of 14 items, and is passed into the getMaxIndex function to find the index of the most probable colour. \bigskip

Since the output of the neural network corresponds to the colour list, with it's guesses in the same order as the colours, the index of the prediction can be used with the colour list to find the predicted colour. This colour is returned.

\paragraph*{getElem\\\\}

This function is used to return the element of a LEGO item from an image of the item. It takes in the image as an input, as well as the element list. \bigskip

It begins by processing the image before it passes it into the neural network. Using the PIL library, the image is grey scaled, and turned into a list of RGB pixel values. The image must be of a dark background, since the neural network was trained on images with dark backgrounds, and I found I got a higher accuracy of predictions if I change the input to fit the neural network rather than train the neural network to work with light background. \bigskip

The background pixel of the image is taken using the very first pixel of the image, the top left corner pixel. If the value is larger than 128, the background is more bright than dark so it is inverted with PIL. If it is larger than 128 and also less than 205, the image is also made darker using PIL to decrease brightness. If the background pixel value is less than 128, and larger than 50, it too is bright so the image is decreased in brightness. What remains is an image with a dark background. \bigskip

Next, the image uses the aspect function from my Augment class of functions to make the image of aspect ratio 1:1. It is resized with PIL thumbnail function to make it 64x64, and the data from the processed image is made into a list. It is normalised so that values are less than 1, by dividing values by 255, and then the list is resized into a 32x32 matrix. The saved Element Neural Network is retrieved, and the image matrix is passed into it. The output is a 10x1 matrix of the probabilities of the image being different elements. \bigskip

The output matrix is turned into a list of 10 items, is passed into the getMaxIndex function to get the index of the most probable element, and then the index is used with the element list to get the element number of the prediction of the neural network, since the neural network's probabilities output is in the same order as the element list. The element number is returned.

\paragraph*{getNumber\\\\}

This function is used to get the LEGO number of an item, from it's colour and element. It takes a colour and element as input, as well as the colour list and ColourElemHashtable. \bigskip

The index position of the colour in the colour list is found, to get a number that represents the colour. This is concatenated to the end of the element number, which too is a string, and this new, longer number is turned into an integer. This is the key. \bigskip

The key is used with the hash function of the hashtable to get an index, and the key and index are both used with the getItem method of the hashtable to find the value, which is the LEGO number. The LEGO number is returned.

\subsection{The Neural Network}

\paragraph*{Neural network overview\\\\}


My solution features the use of machine-learning models for the purpose of identifying a LEGO item from an image, done so by employing two separate neural networks dedicated to the tasks of identifying LEGO element and colour, and then using these pieces of information to find the LEGO number of the item in the image. \bigskip

A neural network is constructed from multiple layers, where data flows through each layer one by one in forward or backward propagation. Implementing the LEGO scanning aspect of my solution will involve implementing these constituent layers, assembling them in a specific order to form the neural network, and finally training the neural network on labelled data so that it may learn.\bigskip

To create the neural network, the different layers will be implemented as classes, all of which inherit from a base Layer class. A NeuralNetwork class will allow for the instantiation of a NeuralNetwork object, which holds layer objects and performs actions on them. I will also be creating a class Matrix Manipulation, 'mm' for short. This class contains all of the functions utilised by the neural network and its layers. \bigskip

\paragraph*{Base Layer Class\\\\}

As mentioned in the \textit{Proposed Solution} section, the fact that the CNN is formed of different layers- some loosely related- makes OOP a great programming paradigm to use in creating the neural network. The different layers may be implemented as different classes, all of which inherit from an abstract class \textit{Layer} that has virtual methods for propagating forwards and backwards. The different children classes can override these methods with their different implementations of propagating, so as to be specialised for their purpose within the neural network. The different types of layers to be implemented are displayed below. \bigskip

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{Layer}   & \multicolumn{1}{c|}{Function}                                                                                                                       \\ [0.5ex] \hline \hline
Hidden &
  \begin{tabular}[c]{@{}l@{}}A layer of neurons. Contains the parameters which we tweak in training that\\  allows the neural network to make classifications.\end{tabular} \\ \hline
ReLU &
  \begin{tabular}[c]{@{}l@{}}Rectified Linear Activation, the activation function. Introduces non-linearity,\\  aids in complex pattern recognition.\end{tabular} \\ \hline
Softmax &
  \begin{tabular}[c]{@{}l@{}}Transforms the raw output of neural network into a probability distribution,\\  essentially gives the probability of all outputs. \end{tabular} \\ \hline
Convolutional &
  \begin{tabular}[c]{@{}l@{}}Extracts different patterns/features from an image, creates "feature maps".\\  These allow for better classification of original image.\end{tabular} \\ \hline
MaxPooling &
  \begin{tabular}[c]{@{}l@{}}Takes feature map from convolutional layer, reduces dimensionality\\  (makes image smaller) to decrease amount of computations required\end{tabular} \\ \hline
\end{tabular}
\end{table}

\bigskip

The Layer class has base attributes for the input and output of the layer, as well as the rate of change of error with respect to input and rate of change of error with respect to output. The base methods include forward propagation, backwards propagation, and getType, which returns the type of layer e.g. returns 'ReLU' when implemented for ReLU class. \bigskip

Please note that I will be referring to the rate of change of error with respect to input as dE\_dI, and rate of chnage of error with respect to output as dE\_dO from here on, as it is in my program code. \bigskip

\begin{center}
\includegraphics[width=16cm]{Images/inheritance.drawio.png}
\end{center}

\subsection{NeuralNetwork class}

The NeuralNetwork class allows for the instantiation of a NeuralNetwork object. This object holds different layer objects in a specific order, tailored to the function of the neural network that the NeuralNetwork object represents. NeuralNetwork objects are named, and have the ability to  saved to and retrieved from a text file the attributes of the layers in the NeuralNetwork object, so that the same neural network may be utilised/trained over multiple program runs. \bigskip

There exists a \textit{has a} relationship between a NeuralNetwork object and the different layer objects that it houses. There is no need for a layer to exist independently of a Neural Network; The layer only has a purpose when it is part of a neural network, and multiple neural networks cannot share the same layer object as the attribute values of the layer will be unique and specialised for the neural network it exists in. Considering this, it would make more sense that the association between the objects is implemented as a \textit{composition} and not \textit{aggregation}. Thus, adding layers to a neural network is implemented as a method in the NeuralNetwork class that instantiates the Layer object, using appropriate data passed in to create it. \bigskip 

\begin{center}
\includegraphics[width=8cm]{Images/nnlayerassoc.drawio.png}
\end{center}

The NeuralNetwork class has a class variable savedNetworks. At the start of the program, the class opens the text file 'savedNetworks.txt', which stores the information needed of a neural network to upload it. This includes it's name, which is the unique identifier of a NeuralNetwork object, the layers in the neural network, and the values for the attributes of the layers. \bigskip

The structure of the text file is as follows:
\begin{center}
    \textit{
    NN1 name\\
    NN1 data\\
    NN2 name\\
    NN2 data\\
    ...
    }
\end{center}
The file saves every neural network over two lines, where the first line contains it's name. The second has a list of data necessary to recreate the saved neural network. Every item in the list is a list of information of a layer in the neural network, in the correct order of the layers. This layer list contains a string of the type of layer, e.g. 'ReLU' if the layer is a ReLU layer, and the rest of the list contains any necessary information on the attributes of the layers, such as weights/biases, needed to recreate the layer. \bigskip

Every line in the text file is stored in the savedNetworks class variable list, so that it looks like [NN1 name, NN1 data, NN2 name, NN2 data, ...]. This list is used by the class to save and retrieve neural networks.

The following class definition gives an overview of the NeuralNetwork class.\bigskip

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Object\\ variables\end{tabular} & Type    & Description                                                                                                                                                                                                                                \\ \hline
Name                                                       & String  & The name of the neural network                                                                                                                                                                                                             \\ \hline
Layers                                                     & List    & \begin{tabular}[c]{@{}l@{}}A list containing all of the layer objects that make up the neural\\ network, in their order\end{tabular}                                                                                                       \\ \hline
LastHidden                                                 & Integer & \begin{tabular}[c]{@{}l@{}}This is the index of the last hidden layer in the neural network, used\\ to initialise that layer's attributes once another hidden layer is added\\ given that it does not have attributes already\end{tabular} \\ \hline
\end{tabular}
\end{table}
\bigskip

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
Method                                                                    & Input/Output                                                                                                 & Description                                                                                                                                                                             \\ \hline
\begin{tabular}[c]{@{}l@{}}view\_saved\\ (class method)\end{tabular}      & none                                                                                                         & Prints the name of every saved neural network                                                                                                                                           \\ \hline
\begin{tabular}[c]{@{}l@{}}retrieve\_saved\\ (static method)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Input: name, new name =\\ None\\ Output: NeuralNetwork\end{tabular}               & \begin{tabular}[c]{@{}l@{}}Returns 'name does not exist' if name is not in\\ savedNetworks. Otherwise, retrieves the saved\\ neural network, with the new name if not None\end{tabular} \\ \hline
addLayer                                                                  & Input: layer info list                                                                                       & \begin{tabular}[c]{@{}l@{}}Creates a layer object using the information in\\ the list, adds the layer to the Layers list of the \\ NeuralNetwork object\end{tabular}                    \\ \hline
\begin{tabular}[c]{@{}l@{}}propagate\_\\ forwards\end{tabular}            & \begin{tabular}[c]{@{}l@{}}Input: input matrix\\ Output: output matrix\end{tabular}                          & \begin{tabular}[c]{@{}l@{}}Uses input to propagate forwards through every\\ layer one by one from the first layer. Output of \\ last layer is returned\end{tabular}                     \\ \hline
\begin{tabular}[c]{@{}l@{}}propagate\_\\ backwards\end{tabular}           & Input:dE\_dO, learning rate                                                                                  & \begin{tabular}[c]{@{}l@{}}Uses dE\_dO and learning rate to propagate\\ backwards through every layer, starting from \\ the last layer.\end{tabular}                                    \\ \hline
train                                                                     & \begin{tabular}[c]{@{}l@{}}Input: training data, results,\\ learning rate, epochs, batch\\ size\end{tabular} & \begin{tabular}[c]{@{}l@{}}Uses the training data and results to train the\\ neural network, using propagate\_forwards\\ and propagate\_backwards\end{tabular}                          \\ \hline
saveNN                                                                    & none                                                                                                         & \begin{tabular}[c]{@{}l@{}}Saves the NeuralNetwork object, to the text file\\ and to the class variable\end{tabular}                                                                    \\ \hline
displayLayers                                                             & none                                                                                                         & \begin{tabular}[c]{@{}l@{}}Prints every layer of the neural network, along\\ with some of the layer's attributes\end{tabular}                                                           \\ \hline
\end{tabular}
\end{table}
\bigskip
The following paragraphs detail some of the methods in greater detail.

\paragraph*{saveNN\\\\}

This method is used to save a neural network. If the name of the object already exists in the savedNetworks list, outputs the message "Network of this name already exists". Otherwise, a list allLayers is created. \bigskip

For every layer in the Layers attribute, a layerInfo list is created. The layer's type is added to the list, using the getType method of the layer object. Next, required attributes of the layer are stored in this list. This is only needed for the hidden, convolutional and max-pooling layer. The following information is also added to this layerInfo list for these layers.
\begin{enumerate}
    \item Hidden: The neurons, weights, and biases of the layer
    \item Convolutional: The input depth, output depth, kernel size, the kernels, and the biases of the layer
    \item Max-pooling: The size and the stride of the pooling kernel.
\end{enumerate}
The layerInfo list is added to the allLayers list. This is done for every layer. Finally, the name of the file and the allLayers list are written to the text file, on separate lines, and both items are appended to the savedNetworks list.

\paragraph*{retrieve\_saved\\\\}

This method retrieves a saved neural network. It is a static method, so does not require an object to call. The name of the neural network to retrieve is taken as an input, with or without a new name. If the name does not exist in the savedNetworks list, then this message is returned. Otherwise, proceeds. \bigskip

It gets the index of the name in the list, and gets the neural network data by getting the item from the list at the position of index+1, since it is stored right after the name. The data is evaluated, turning it from a string to list. A NeuralNetwork object is created, with the same name as the retrieved neural network if no new name was input, otherwise with the new name. Then, for every item in the data list, the addLayer method is called to add the layer to the NeuralNetwork object. Finally, this object is returned.

\paragraph*{addLayer\\\\}

This method is used to add a layer to the neural network, from a list containing information on the layer. This might not contain all of the attributes of the layer, for example if a neural network is being created for the first time as opposed to being retrieved. In this scenario, these attributes will have to be initialised.\bigskip

The first item from the list is used to determine the type of the layer. Depending on what type it is, that layer object is created.
\begin{enumerate}
    \item Hidden: A Hidden object is created. The second item in the list is set as the number of neurons attribute for the object. If the length of the list is longer than 2, then the third item is set as the object's weights attribute, and the third the biases attribute. Weights and biases of a neural network are of the layer after it, so the last hidden layer does not need these attributes, it is merely an output layer. Thus, when a new hidden layer is added, it is the \textit{previous} hidden layer whose attributes need initialising. \bigskip

    This layer is gotten using the NeuralNetwork's LastHidden attribute, which has the index of the last hidden layer of the set. If it is None, then the hidden layer being added is the first hidden layer of the neural network, so nothing needs to happen. Otherwise, the layer object of the previous hidden layer is gotten using the Layers list and the index. This layer has its biases initialised as a matrix of number of neurons of layer being added (second item of list) by 1. Every value in this matrix is set at 0.01. Next, the objects weights are initialised as a matrix of number of neurons of layer being added (second item of list) by number of neurons of previous hidden layer, gotten by using getNeurons on the previous layer. The values are initialised using Xavier initialisation. A variable x is set to 1/(number of neurons of previous layer). Values of the weights matrix are generated randomly, uniform between a range of -x to x. This initialisation optimises the neural network. The matrices are made using the mm.makematrix function, part of my Matrix Manipulation class of functions that will be covered later. \bigskip

    Finally, the newly added hidden layer is set as the last hidden layer, by setting the LastHidden attribute to the length of the Layers list (this is before current layer is added to it, so represents the index that the current layer will have in the Layers list).

    \item Convolutional: The second, third, fourth and fifth items of the list are used to instantiate a Convolutional object. If the length of the list is greater than 5, then the object's kernels attribute is set at item 6 of the list, and biases set at item 7. Otherwise, these need to be initialised for the layer object. Using the other information in the list, the dimensions of the bias are worked out, and a matrix of these dimensions is made using mm.makematrix. The vales of the items is set at 0.01. The same goes for the kernels, for each kernel a matrix is created of the calculated dimensions, and the value is randomly generated using a Gaussian distribution, with a mean of 0 and standard distribution of 0.01. The biases and weights attributes of the layer object are set to these matrices. 

    \item ReLU: A ReLU object is created.
    \item Softmax: A Softmax object is created.
    \item MaxPooling: A MaxPooling object is instantiated passing in the second and third items of the list, the size and stride.
    \item Anything else: returns "Invalid layer type"
\end{enumerate}
Finally, the created layer object is added to the set's Layers list.

\paragraph*{train\\\\}

This method is used to train the neural network on a list of data, with it's results list. The method takes in data and results, a learning rate, epoch and a batch size. The data represents the items of a dataset to be trained with, with the labels of the data in the results list, at the same indexes. A learning rate is how extreme changes are made to parameters with a backwards pass. An 'epoch' is how many times the whole dataset is trained over. Therefore, for the number of epochs taken in as an argument, the whole dataset is trained over. \bigskip

The batch size refers to the number of data-value pairs worked through before a back-propagation is made. For example, if the batch size is 1 then after every forward propagation there is a backwards propagation. If the batch size is \textit{n}, then there is one backwards propagation for every \textit{n} forwards propagation. The dE\_dO used to propagate backwards is the average dE\_dO of every forwards propagation in the batch. \bigskip


First, the dimensions of a result (rx,ry) are found, by using the mm.dim function on the first item of the results list. Next, for the number of epochs, the dataset is trained with. For each epoch, the dataset must be shuffled, to prevent over-fitting. This is achieved using the mm.testrand function, which takes in the length of the dataset and returns a list of items from 0 to the length-1, randomly shuffled. These are used as indexes: for every number in the list, an item from the data and results are used. This effectively allows for the order in which data-result pairs are trained with to be different every time. \bigskip

At the start of the epoch, a totalError matrix and temp matrix is initialised using mm.makematrix, both of dimensions (rx,ry) and values set at 0. A count is initialised at 0, and then shuffled dataset is trained with. For every data-result pair, the count is incremented by 1. The data is used to propagate forwards through the neural network, and gets an output. The output is used to find the mean square error, calculated using the output and the result with the mm.msError function. This mean square error is added onto the totalError using mm.summatrix. The dE\_dO is calculated also using the output and result, and this gets added to the temp matrix. \bigskip

If the count mod batch size is 0, then a back propagation is made. The temp matrix is the sum of the dE\_dO over the batch. Dividing the values by the batch size therefore gives the average dE\_dO of the batch. This is done using mm.scalematrix, scaling the temp matrix by 1/(batch size). The calculated average dE\_dO is used along with the learning rate to propagate backwards through the neural network. The values in the temp matrix are set back to zero, for the next batch. This process is repeated for every item in the dataset. \bigskip

After every item in the dataset is trained with, the program outputs the epoch number. All of the numbers in totalError is summed to get a single number, which is displayed after the epoch number. This number is useful information as it is an indicator of progress. As the neural network trains, the errors of every forward propagation gets smaller, so with every epoch the totalError is smaller so the sum of it's values is smaller too. The rate at which it decreases, and how much it deceases by over multiple epochs shows how well the model is learning. \bigskip

After the epoch and error numbers are output, the next epoch begins, until finally all epochs have been trained over.

\subsection{Matrix Manipulation class}

The Matrix Manipulation class, mm for short, is a class of functions used to perform actions with matrices, used by the neural network and the layer classes. The functions are implemented as static methods so that an instantiated object is not required to call the functions. \bigskip

The following table gives an overview of all of the functions in the class.

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
Method       & Input/Output                                                                                                 & Description                                                                                                                                                                                                                                                                                                                                                                                                                       \\ \hline
makematrix   & \begin{tabular}[c]{@{}l@{}}Input: rows, columns, \\ num = None, gauss=None\\ Output: new matrix\end{tabular} & \begin{tabular}[c]{@{}l@{}}Creates a matrix of dimensions rows x columns.\\ Values of  items set at num. If num is a list and \\ gauss is None, then values generated randomly,\\ between 1st and 2nd item of list. If num is a list\\ and gauss is not None, then values generated \\ with gaussian distribution, with the mean as 1st\\ item of list and standard deviation as the 2nd.\\ Returns this new matrix.\end{tabular} \\ \hline
prod         & \begin{tabular}[c]{@{}l@{}}Input: matrix a, matrix b\\ Output: new matrix\end{tabular}                       & Returns the product of two matrices                                                                                                                                                                                                                                                                                                                                                                                               \\ \hline
summatrix    & \begin{tabular}[c]{@{}l@{}}Input: matrix a, matrix b\\ Output: new matrix\end{tabular}                       & Returns the sum two matrices                                                                                                                                                                                                                                                                                                                                                                                                      \\ \hline
submatrix    & \begin{tabular}[c]{@{}l@{}}Input: matrix a, matrix b\\ Output: new matrix\end{tabular}                       & Returns the difference of two matrices                                                                                                                                                                                                                                                                                                                                                                                            \\ \hline
scalematrix  & \begin{tabular}[c]{@{}l@{}}Input: matrix, scale factor\\ Output: new matrix\end{tabular}                     & \begin{tabular}[c]{@{}l@{}}Scales a matrix by the scale factor. Returns this\\ new matrix\end{tabular}                                                                                                                                                                                                                                                                                                                            \\ \hline
raise\_power & \begin{tabular}[c]{@{}l@{}}Input: matrix, power\\ Output: new matrix\end{tabular}                            & \begin{tabular}[c]{@{}l@{}}Raises every item of a matrix to a power.\\ Returns this new matrix\end{tabular}                                                                                                                                                                                                                                                                                                                       \\ \hline
hadamard     & \begin{tabular}[c]{@{}l@{}}Input: matrix a, matrix b\\ Output: new matrix\end{tabular}                       & Returns the Hadamard product of two matrices                                                                                                                                                                                                                                                                                                                                                                                      \\ \hline
transpose    & \begin{tabular}[c]{@{}l@{}}Input: matrix\\ Output: new matrix\end{tabular}                                   & Transposes a matrix. Returns this new matrix                                                                                                                                                                                                                                                                                                                                                                                      \\ \hline
msError      & \begin{tabular}[c]{@{}l@{}}Input: matrix a, matrix b\\ Output: new matrix\end{tabular}                       & \begin{tabular}[c]{@{}l@{}}Returns the mean-square error between two\\ matrices\end{tabular}                                                                                                                                                                                                                                                                                                                                      \\ \hline
dE\_dO       & \begin{tabular}[c]{@{}l@{}}Input: matrix a, matrix b\\ Output: new matrix\end{tabular}                       & \begin{tabular}[c]{@{}l@{}}Returns the rate of change of error with respect\\ to output. Used for back-propagation\end{tabular}                                                                                                                                                                                                                                                                                                   \\ \hline
dim          & \begin{tabular}[c]{@{}l@{}}Input: matrix\\ Output: rows, columns\end{tabular}                                & Returns the dimensions of a matrix                                                                                                                                                                                                                                                                                                                                                                                                \\ \hline
alter\_dim   & \begin{tabular}[c]{@{}l@{}}Input: matrix, new\\ dimensions\\ Output: new matrix\end{tabular}                 & \begin{tabular}[c]{@{}l@{}}Alters the dimensions of matrix to make it 1 x n\\ or n x 1. Returns this new matrix\end{tabular}                                                                                                                                                                                                                                                                                                      \\ \hline
copy         & \begin{tabular}[c]{@{}l@{}}Input: matrix\\ Output: new matrix\end{tabular}                                   & \begin{tabular}[c]{@{}l@{}}Returns a copy of the matrix. Copying a matrix\\ is not a feature in python like it is with a list\end{tabular}                                                                                                                                                                                                                                                                                        \\ \hline
testrand     & \begin{tabular}[c]{@{}l@{}}Input: size\\ Output: list\end{tabular}                                           & \begin{tabular}[c]{@{}l@{}}Returns a list of numbers from 0 to size-1,\\ randomly shuffled. Used for training NN\end{tabular}                                                                                                                                                                                                                                                                                                     \\ \hline
identity     & \begin{tabular}[c]{@{}l@{}}Input: size\\ Output: matrix\end{tabular}                                         & \begin{tabular}[c]{@{}l@{}}Returns an identity matrix of dimensions \\ size x size\end{tabular}                                                                                                                                                                                                                                                                                                                                   \\ \hline
convolve     & \begin{tabular}[c]{@{}l@{}}Input: matrix, kernel, full=\\ None\\ Output: new matrix\end{tabular}             & \begin{tabular}[c]{@{}l@{}}Convolves a matrix by a kernel. Does valid\\ convolution, unless full is not None. Returns the\\ resulting matrix\end{tabular}                                                                                                                                                                                                                                                                         \\ \hline
rot          & \begin{tabular}[c]{@{}l@{}}Input: matrix\\ Output: new matrix\end{tabular}                                   & Rotates a matrix. Returns the new matrix                                                                                                                                                                                                                                                                                                                                                                                          \\ \hline
pool         & \begin{tabular}[c]{@{}l@{}}Input: matrix, size, stride,\\ mode\\ Output: new matrix, list\end{tabular}       & \begin{tabular}[c]{@{}l@{}}Pools a matrix using a kernel of dimensions\\ size x size and with specified stride. Mode\\ determines whether pool is min, max, or average.\\ Returns resulting matrix, as well as a list of the \\ indices of the max pixels if mode is max\end{tabular}                                                                                                                                             \\ \hline
\end{tabular}
\end{table}
\bigskip

The convolve and pool functions will be explained in greater depth with the Convolutional layer and MaxPooling layer respectively. \bigskip

\subsection{Back-propagation}

During training, after an input has propagated forwards through every layer, the final output of the neural network, the output of the last layer, is compared against the expected result, and a cost/error is calculated. This cost represents how off the output of the neural network is from the correct result. \bigskip

When a back-propagation occurs depends on the type of gradient descent. There are three types:
\begin{enumerate}
    \item Batch gradient descent: Every item in the dataset is first propagated-forwards with before a back-propagation occurs
    \item Mini-batch gradient descent: A back-propagation occurs after a fixed number of items in the dataset are propagated-forwards with. This fixed number is the 'batch-size'
    \item Stochastic gradient descent: A back-propagation occurs after every single item in the dataset is propagated forwards through
\end{enumerate}
My NeuralNetwork's train method is implemented as taking in a 'batch-size'. This essentially allows any of the above three gradient descent methods displayed above to be used. Stochastic gradient descent is achieved by setting the batch size to 1, batch gradient descent is achieved by setting the batch size to the total number of items in the dataset, and any other value for the batch size between these two results in a mini-batch gradient descent. In my neural networks, I use a batch size of 1 as I find stochastic gradient descent gives me the best results for my dataset. \bigskip

Back-propagation is already explained in the Analysis section, but to summarise, it involves updating the model parameters using the rates of change of the parameters with respect to the cost, in an attempt to decrease the cost function. The gradient is calculated, and is used to move down the gradient in an attempt to reach it's minimum point- hence the term gradient descent. One factor that affects the convergence is the learning rate, which is how drastically the changes are made. A higher learning rate results in bigger changes which could lead to travelling towards the minimum point faster, but it runs the risk of moving too drastically and overshooting the minimum point. Learning rate is something that must be determined and tweaked heuristically/ through trial and error to achieve the optimal convergence of the neural networks I create. \bigskip

In order for back-propagation to happen, however, the error must first be found, after with the rate of change of the error with respect to the output of the final layer must be found. This dE\_dO is passed into the last layer, used to update any of it's parameters, and used to find the rate of change of the error with respect to the \textit{input} of the layer. This dE\_dI is the dE\_dO of the \textit{previous} layer, since the output of one layer is the input of the next when forwards propagation occurs. Finding the dE\_dI of a layer finds the dE\_dO of the previous layer, which allow the previous layer to update it's parameters to decrease error. \bigskip

There are different methods of finding error. My solution uses the mean square error. The MSE finds the difference between the ouput matrix and true result matrix, squares the values in the matrix so that they are all positive, and divides these values by the total number of items in the matrix, hence why it is called the mean square error. This can be represented by the following formula.\footnote{\url{https://www.bragitoff.com/2021/12/mean-squared-error-loss-function-and-its-gradient-derivative-for-a-batch-of-inputs-python-code/}}
\[\frac{1}{N}\sum_{i=1}^{N}(y_{i}-\hat{y}_{i})^{2}\]
This is implemented in my solution using the mm.msError function, which takes in the output and true result matrices as input, uses mm.submatrix to find the difference, uses mm.raise\_power with 2 input to square every element, uses mm.dim on the output to find the number of rows, representing the number of items, and finally uses mm.scalematrix to scale the squared difference by 1/(number of rows), to divide every term in the matrix by the total number of terms. The msError function is used in the train method of the NeuralNetwork to find the total error over an epoch, to output as the neural network trains, as an indicator of it's learning progress. \bigskip

The actual back-propagation does not make use of the MSE, but of its derivative. This can be calculated using the chain rule. Its formula looks like this.
\[\frac{2}{N}\sum_{i=1}^{N}(y_{i}-\hat{y}_{i})\]
This is implemented in my solution with the mm.dE\_dO function, which takes in the output and true result matrices as input, uses mm.submatrix to find the difference, uses mm.dim on the output to find the number of rows, representing the number of items, and finally uses mm.scalematrix to scale the difference by 2/(number of rows). The dE\_dO function is used in the train method of the NeuralNetwork, where it sums the dE\_dOs of every forward-propagation in a batch, and finds the mean dE\_dO by diving the total by the batch size, which it uses to propagate backwards, for every batch.

\subsection{Hidden class}

The Hidden class is a class which inherits from the Layer class, used to implement a hidden layer.

\paragraph*{Specialised attributes\\\\}

The hidden layer is a layer of neurons. Every neuron in the layer is connected to every neuron in a hidden layer before/after it, and every neuron has a bias attached to it. \bigskip

Forward propagation involves taking an input matrix, multiplying it by the weights matrix, and adding the bias matrix, to get the output matrix. The biases added on are the biases attached to the next hidden layer's neurons. Therefore, I implemented an attribute for the 'next layer biases', rather than for the current layer's biases. This is because the layer does not make use of it's own biases, so storing it in an attribute would be wasteful, and because it does make use of the next layer's biases, which would require having to constantly fetch the biases of the next hidden layer, increasing computations. It only therefore makes sense to have an attribute for the next hidden layer's biases. \bigskip

The same logic can be applied to the weights matrix, a hidden layer only makes use of the weights it has with the hidden layer after it and not before it, so the weights attributes will store the weights matrix of the weights before the current layer and the layer that comes after it. \bigskip

Therefore, these attributes default as None, since adding a hidden layer to the neural network makes it the last hidden layer in that neural network, there are no next-layer biases or weights with the next layer. However, when another hidden layer \textit{does} get added after this, then the weights and biases do exist for it, so adding a hidden layer to a neural network initialises the weights and biases attributes of the previous hidden layer. This initialisation requires the number of neurons of both neural networks to get the dimensions of the weights and biases matrices correct, and the values are initialised with Xavier initialisation for optimisation. This is further explained in the NeuralNetwork class section where it is explained in the addLayer paragraph. \bigskip

Overall, the specialised attributes of this class are the weights, the next layer biases which I will call the NLbiases, and the number of neurons of the layer. The layer will also need the an attribute for the matrix shape of the input.

\paragraph*{Forward propagation\\\\}

Before the layer can propagate forwards, it must ensure the input matrix is flattened, so that it is only 2 dimensional. This is not the case if the layer before it was convolutional/max-pooling. In this case, it flattens the input, and makes a note of the original input dimensions. \bigskip

Next, the layer checks if the weights matrix is None. If it is, then the layer is the last layer of the neural network, so it simply returns the input. If it does, then finds the output using the formula \[Y = WX + B\]where Y is the output, W is the weights matrix, X is the input and B is the biases matrix. The output is calculated using mm.prod to multiply the weights by the input, and using mm.summatrix on the result of this and the biases. The input attribute of the layer is set at the input using mm.copy, as this will be needed in the back-propagation. Finally, the output is returned.

\paragraph*{Backwards propagation\\\\}

If the weights matrix is None, then the layer is the last layer in the neural network, so the dE\_dI is simply the dE\_dO. This is returned. \bigskip

Otherwise, the layer must update it's weights and biases. It needs to find the rate of change of error with respect to respect to weights, with respect to biases, and with respect to the input, which I will refer to as dE\_dW, dE\_dB and dE\_dI respectively. Formulae for these can be derived using the chain rule.\footnote{\url{http://cs231n.stanford.edu/handouts/linear-backprop.pdf}} \bigskip

The dE\_dW can be found with the formula \[\frac{\partial E}{\partial W} = X^{T}\frac{\partial E}{\partial O}\]where X is the input from the forward propagation that was stored to the input attribute of the layer. In other words, the the dE\_dW is equal to the input matrix transposed multiplied by the the dE\_dO. This is implemented using mm.transpose on the input gotten from the input attribute, and multiplied to the dE\_dO with mm.prod. The resulting dE\_dW is then scaled by the learning rate using mm.scalematrix before it is subtracted from the layer's weights using mm.submatrix. \bigskip

The dE\_dB can be found with the formula \[\frac{\partial E}{\partial B} = \frac{\partial E}{\partial O}\]It is simply equal to the dE\_dO. Therefore, the result of scaling the dE\_dO by the learning rate with mm.scalematrix is subtracted from the layer's biases using mm.submatrix. \bigskip

The dE\_dI can be found with the formula \[\frac{\partial E}{\partial I} = \frac{\partial E}{\partial O}W^{T}\]In other words, it is the product of the dE\_dO with the transposed matrix of the weights. A matrix of the transposed weights is gotten using mm.transpose on the weights, which is multiplied to the dE\_dO using mm.prod with the dE\_dO in front, as shown in the formula. \bigskip

If the input matrix shape attribute is None, then the input of the layer did not have to be flattened when it was passed in, so the dE\_dI can simply be returned. It it is not None, then the dE\_dI must be made 3D, into the same dimensions as those stored by the attribute. The dE\_dI is made into a matrix of this shape, before being returned.

\subsection{ReLU class}

The ReLU class is a class which inherits from the Layers class, used to implement a ReLU activation layer.

\paragraph*{Specialised attributes\\\\}

The ReLU activation is used after both the hidden layer and the convolutional layer. The activation function must be implemented in a way to work with both of these, since the output of the hidden layer is 2D, while the output of the convolutional layer is 3D. \bigskip

My solution to this problem is to make the ReLU work with 3D matrices, and deal with 2D matrices by turning them 3D, simply adding the matrix to an empty list to turn it from \textit{a x b} to \textit{1 x a x b}. At the end, if it was 2D at the start then the matrix to be returned is made 2D as well. ReLU objects have a boolean attribute is3D, set to True if the input to the layer is 3D and False if it is 2D. \bigskip

\paragraph*{Forward propagation\\\\}

If the first item of the first item of the input is a list, then the input is 3D. The attribute is3D is set to True. Otherwise it is set to False, in which case the input is made 3D by adding it to an empty list. The input is now saved to the input attribute of the layer, to be used for back propagation. \bigskip

The ReLU activation function introduces non-linearity into the neural network by getting rid of all negative values in the input, making them zero. In prototyping I found this to be troublesome because of the 'dying ReLU' problem, which I solved by implementing instead a 'leaky ReLU'. The leaky ReLU . The leaky ReLU does not completely kill negative values, but instead makes them very small, generally multiplying them by 0.01. This worked a lot better for me. \bigskip

For example, given the following matrix
\begin{center}
    $ \begin{bmatrix}
    1 & 5 & -3 \\
    7 & 2 & -1 \\
    6 & -12 & 2 
    \end{bmatrix}  $
\end{center}
The output matrix should be 
\begin{center}
    $ \begin{bmatrix}
    1 & 5 & -0.03 \\
    7 & 2 & -0.01 \\
    6 & -0.12 & 2 
    \end{bmatrix}  $
\end{center}
\bigskip

This is implemented in my solution by applying the leaky ReLU function to every value in the matrix
\[f(z) = max(0.01z, z)\]If the value is positive, the max is the value itself, so it does not change. If it is negative however, then the max is the value x 0.01, so this replaces it. \bigskip

Finally, if is3D is True, then the matrix is returned. Otherwise, the matrix is made 2D again by taking the first and only item out of the matrix, and returning this 2D matrix.

\paragraph*{Backwards propagation\\\\}

The back-propagation of a ReLU works by multiplying values of the dE\_dO by 0.01 at the same positions where they were done so to the input, to find the dE\_dI. For instance, following on from the example in the forward propagation, take the following matrix of dE\_dO
\begin{center}
    $ \begin{bmatrix}
    0.3 & 34 & -36 \\
    89 & 6 & -4.4 \\
    0.1 & -112 & 24 
    \end{bmatrix}  $
\end{center}
Then assuming the first matrix from the example in the forwards propagation is the input, the dE\_dI is 
\begin{center}
    $ \begin{bmatrix}
    0.3 & 34 & -0.36 \\
    89 & 6 & -0.044 \\
    0.1 & -1.12 & 24 
    \end{bmatrix}  $
\end{center}
since in the input it is the 3rd number on the first and second row, and the 2nd number on the last row which has changed, so the numbers in these positions in the dE\_dO are also multiplied by 0.01 to get the dE\_dI. \bigskip

The input is retrieved from the input attribute of the layer. The back-propagation method is also designed to work with 3D matrices, so if the dE\_dO is 2D and is3D is True, then the dE\_dO is made 3D by adding it to an empty list. \bigskip

Next, a temporary matrix is created based off of the input matrix. For every value in the input matrix, if it is positive then a value of 1 is put into this position in the temp matrix. Otherwise, if it is negative, then a value of 0.01 is put in the temp matrix. What results is a matrix that has a 1 where the input matrix would have a positive number, and a 0.01 where the input matrix would have a negative number. Finally, the dE\_dI is found by computing the element-wise product of this temp matrix and the dE\_dO matrix, using mm.hadamard. \bigskip

If is3D is True, then the dE\_dI is returned. Otherwise, the dE\_dI is made 2D by getting the first and only item from the matrix. This is returned.

\subsection{Test 1- XOR}

At this stage, to test the hidden and ReLU layers, I trained a neural network that has only hidden and ReLU layers to take in an two numbers, both either a 0 or 1, and output a 1 or 0, essentially performing an XOR operation on the two inputs. \bigskip

The dataset for this is simple. The training data is a list containing all possible inputs of the XOR function, and the training results is a list containing the labels of the data, the outputs of the XOR. The label of the first item of the results corresponds to the first item of the data, the second label corresponds to the second piece of data, etc. \bigskip

The architecture of the neural network created for this task consists of a hidden layer of 2 neurons, to take in the two input numbers, a ReLU layer, a hidden layer of 10 neurons, a ReLU layer, a hidden layer of 10 neurons, a ReLU layer, and finally a hidden layer of 1 neuron to output the single number result. \bigskip

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=17cm]{Images/XORNN.drawio.png}}%
\end{figure}
\bigskip

Because this task is so simple, the neural network can be made very small and simple, and training is fast so many epochs can be trained over. Thus, I trained the neural network on the training data and training results, at 10,000 epochs, batch size of 1, and learning rate of 0.001. \bigskip

I then tested the neural network, passing in the inputs of an XOR and comparing the results to the expected results of an XOR. I re-initialised and re-trained the neural network like this multiple times, to see how it performed on each time. Generally the results were good, equal to 0 or 1 respectively to an extremely close approximation. Adding a softmax layer to the end would allow it to classify images between either 0 or 1, and would undoubtedly yield a near perfect accuracy. However, there was a glaring issue: many of the trainings resulted in very poor results, where it would only output values equal to 0 to an extremely close approximation. After much research, I had determined that the issue was a result of the 'dying ReLU problem'. This is a problem where a neural network outputs only 0s. I found that a solution to this problem is implementing a leaky ReLU instead of a normal ReLU. Prior to this test, I had implemented a normal ReLU, which replaces negative values with a zero, so tweaked it to be a leaky ReLU which instead replaced negative values with the value multiplied by 0.01. Furthermore, I implemented Xavier initialisation for the hidden layers to ensure the best chance of convergence to the desired results and further prevent the risk of dying neurons in the neural network over training. \bigskip

Making these two fixes to the layers eliminated the dying ReLU problem and improved my neural network significantly.


\subsection{Softmax class}

The Softmax class is a class which inherits from the Layers class, used to implement a softmax activation layer. This is the final layer in a neural network, and the layer's purpose is to turn the raw output of the neural network into a vector of probabilities, for predictions to be made. \bigskip

\paragraph*{Forward propagation\\\\}

The forward propagation of the softmax layer simply applies the softmax function to the input matrix. \[\sigma(z_i) = \frac{e^{z_{i}}}{\sum_{j=1}^K e^{z_{j}}}\]
\bigskip
demonstrate this function, take the the matrix 
\begin{center}
    $ \begin{bmatrix}
    1 \\
    2 \\
    3 
    \end{bmatrix}  $
\end{center}
Applying the softmax function to the matrix would give
\begin{center}
\begin{equation*}
    \begin{bmatrix}
    \frac{e^{1}}{e^{2}+e^{3}+e^{4}} \\\\
    \frac{e^{2}}{e^{2}+e^{3}+e^{4}} \\\\
    \frac{e^{3}}{e^{2}+e^{3}+e^{4}} 
    \end{bmatrix}
\end{equation*}
\end{center}
which could simplify into
\begin{center}
    $ \begin{bmatrix}
    0.0900 \\
    0.2447 \\
    0.6652 
    \end{bmatrix}  $
\end{center}
\bigskip

The forward propagation method applies this using map and lambda functions, to find e to the power of every item, find the sum of these, and divide all the items by this sum. The output is copied to the output attribute, as it is needed in the back-propagation, and the output is returned.

\paragraph*{Backwards propagation\\\\}

The layer only needs to find the dE\_dI, however the formula to do so is very complicated. Luckily, it can be simplified to the formula \footnote{Derived by The Independent Code at 5:45 in the following video: \url{https://www.youtube.com/watch?v=AbLvJVwySEo}}
\begin{center}
\begin{equation*}
    (M\circledcirc (I-M^{T}))\cdot \frac{\partial E}{Y}\; \; \; \; \; \; \; M=\begin{bmatrix}
    y_{1} &  y_{1} &  \cdots & y_{1} \\ 
    y_{2} &  y_{2} &  \cdots & y_{1}\\ 
     \vdots & \vdots  & \ddots  &\vdots  \\ 
    y_{n} &  y_{n}&  \cdots & y_{n}
    \end{bmatrix}
\end{equation*}
\end{center}
where \textit{I} represents the identity matrix, the \(\circledcirc\) symbol refers to an element-wise product, and\(\frac{\partial E}{Y}\) is just the dE\_dO. \bigskip

This formula makes the back propagation a lot easier to implement. The output of the forwards propagation is retrieved from the output attribute, and this is used to create the M matrix as is shown in the formula, with the item in every row repeated for the number of rows such that we are left with a matrix of size n x n. \bigskip

The mm.identity function is used, with the length of the output matrix passed in as an argument, to create an identity matrix that is also of size n x n. A transposed M matrix is made using the mm.transpose function on M, and this is subtracted from the identity matrix using mm.submatrix to create a temp matrix. After this, the mm.hadamard function is used to find the element-wise product of the temp matrix with the M matrix, and finally mm.prod is used to find the product of this with the dE\_dO matrix. What is left is the dE\_dI, which is returned.

\subsection{Test 2: MNIST}

The MNIST dataset is a dataset of handwritten digits, all labelled, to be used for training a neural network to recognise the digits from the images. \bigskip

I downloaded a file of the dataset data as comma seperated values, with each pixel represented as it's RGB value so that I would not need to do this pre-processing myself. I created an MNISTproc function to fully process this data to be trained with by my neural network, to store the data for each image in a training data list, and for the corresponding label of the image to be stored in a training results list. The training data is normalised in this processing, by dividing all values by 255 so that every item has a value between only 0 and 1. This processing of the data takes very long given there are 60000 images, so at the end of the processing I stored these two  lists in a text file. They are retrieved by a retProc function, that simply reads the lists from the text file that stores them. This still takes long to finish, but is a lot quicker than processing the data every time. Upon retrieving the lists of data and results, the lists are split so that the first 50000 images could be used for training and the last 10000 for testing. \bigskip

Each image is 28 by 28 pixels, resulting in 784 pixels to be taken in as input, and classifies images between 10 different digits, so outputs 10 numbers. Thus, the neural network begins with a hidden layer of 784 neurons and ends with a softmax activation layer followed by a hidden layer of 10 neurons to produce the output. As for the rest of the layers between them, I experimented with multiple different architectures and different values of learning rate, epochs and batch sizes. At the end of training I would test the neural network with an MNIST\_TEST function, which tests the neural network on every item in the testing data and finds it's percentage accuracy. Every neural network attempt resulted in poor results. \bigskip

However, after many attempts, I was able to create a neural network that worked. It begins with a hidden layer of 784 neurons, a ReLU layer, a hidden layer of 10 neurons, a ReLU layer, a softmax layer and a hidden layer of 10 neurons. This is an extremely simply architecture; prior attempts involved much larger and more complicated neural network, which I assumed would be necessary for the task. I trained the data on 100 epochs, which is a lot more than I expected I would need, a batch size of 1, and a learning rate of 0.01, with only the first 1000 images from the dataset used instead of the whole 50000. \bigskip

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=17cm]{Images/MNIST.drawio.png}}%
\end{figure}
\bigskip

This made me realise the importance of choosing the optimal architecture and best values for hyper-parameters when training a neural network. I am limited in my computational resources and time to train a neural network, so the best solution is one that balances both training time and performance. The neural network created, despite being smaller and more simple than I expected, yielded a accuracy of 86.22\%. 

\subsection{Convolutional class}

The Convolutional class is a class which inherits from the Layers class, used to implement a convolutional layer. The purpose of the convolutional layer is to create feature maps of the image, which leads to better image recognition. Feature maps are created using a kernel to slide over the image, in a process known as a convolution. Performing a convolution is implemented in my solution using the mm.convolve function.

\paragraph*{Convolve function\\\\}

The mm.convolve function takes in the matrix and kernel as input. It also may take in an input to set the convolution option from 'valid' to 'full', otherwise the convolution is defaulted as a valid convolution. \bigskip

The function begins by getting the dimensions of the matrix and of the kernel, using the mm.dim function. \bigskip

If the convolution option is set to full, then a full convolution must be performed. A valid convolution involves sliding the kernel over the image starting with the top left item of the kernel on the top left item of the matrix, and the kernel is slid such that the entire kernel is always overlapping the matrix. A full convolution, on the other hand, begins with the bottom right item of the kernel on the top left item of the matrix, and is slid such that a part of -but not necessarily the whole- of the kernel is always overlapping the matrix. \bigskip

The full convolution and valid convolution can be implemented using the same algorithm, that of a valid convolution. To perform a full convolution, the matrix can simply be given zero padding on the edges. Thus, if the option is 'full', then the function adds zero padding to the edges of the input matrix, the amount of which is calculated such that the number of rows of the matrix are now ((rows of matrix) + 2(rows of kernel) - 2), and like wise for the new number of columns. \bigskip

Next, it begins the convolution. An empty list is created for the output matrix. All of the positions that the top left corner of the kernel can be placed onto the matrix exist where the first ((rows of matrix) - (rows of kernel) + 1) rows and first ((columns of matrix) - (columns of kernel) + 1) columns of the matrix overlap. These positions are accessed using a nested for loop.
\bigskip

For each row in range ((rows of matrix) - (rows of kernel) + 1), an empty list is created for the row of the output. Then, for each column in range ((columns of matrix) - (columns of kernel) + 1), the component of the matrix that has a top left item at this row and column, and has dimensions equal to the dimensions of the kernel, is found. This subspace of the matrix, the area over which the kernel overlaps the matrix, undergoes an element-wise multiplication with the kernel, and every value from this multiplication is summed together and appended to the output row list. After every column in the range has been cycled through, the output row list is appended to the output matrix list. After all of the rows in the range have been looped through, the convolution is complete, and the output matrix is returned.

\paragraph*{Specialised attributes\\\\}

The convolutional layer has attributes of input depth, output depth, input size, kernel size, the kernels themselves and the biases. The kernels and biases are initialised for the layer when it is added to a NeuralNetwork with the addLayer method, with the dimensions made in accordance to the size and depth of the kernels. The biases are initialised with values 0.01, and the kernel's values are initialised to be a random following a Gaussian distribution of mean 0 and variance 0.01. \bigskip


The convolutional layer takes in 3D input, a list of 2D matrices, as input. The number of 2D matrices in the list is referred to as the depth of the input. The size of the input refers to the dimensions of one of these 2D matrices, and the size of the kernels is the dimension required for the kernel. The output depth is how many 2D matrices should be output. The depth of the biases is equal to the output depth. The kernels is a list of lists, where every list inside contains kernels. The number of kernels in each list is equal to the input depth, and the number of these lists is equal to the output depth. 

\paragraph*{Forward propagation\\\\}

The goal of the forward propagation is to use the kernels and biases, the parameters of the convolutional layer, to turn the input into an output. Every matrix contained in the output is a feature map created from performing convolutions. To create the first of these feature maps, the first list in the kernels is used. Every kernel is this list undergoes a convolution with a matrix from the input using mm.convolve, such that the first kernel from the list is convoluted with the first matrix in the in the input, the second kernel from the list is convoluted with the second matrix in the input, etc. The sum of all of these is found, and added to the first item in the list of biases. What remains is the first matrix of the output. \bigskip

The forward propagation method of the Convolutional class implements this. First, the input is stored to the input attribute of the class, as it will be needed for back-propagation. Next, an empty output list is created, which will contain all of the output matrices. For an index in range of the output depth, an output matrix is created. A temp matrix is made equal to the bias matrix at the index of the biases attribute. Next, the input is zipped with the list of kernels at the index from the kernels attribute. This pairs up the input matrices and the kernels in this kernel list. A lambda function is mapped to every item in this list of pairs to apply the mm.convolve function on every pair item, taking the input matrix as the input and the kernel matrix as the kernel. This results in a list of feature maps. This list is looped through to add every one of these feature maps to the temp matrix using mm.summatrix, and the remaining matrix is appended to the output list. \bigskip

Finally, once every output matrix has been added to the output list, the output list is returned.

\paragraph*{Backwards propagation\\\\}

The backwards propagation involves updating the kernels, updating the biases, and finding the dE\_dI. Updating the kernels and biases involves finding the dE\_dK and dE\_dB. These can be found using their respective formulas.\footnote{These too were derived by The Independent Code in the following video: \url{https://www.youtube.com/watch?v=Lakz2MoHy6o}}\bigskip

The dE\_dK is found with the formula
\begin{equation*}
\frac{\partial E}{\partial K_{ij}} = X_{j} \star \frac{\partial E}{\partial Y_{i}}
\end{equation*}

This essentially means that the nth list of matrices in the dE\_dK is found by finding the convolutions of every matrix from the input of the forward propagation with the nth matrix of the dE\_dO. Each of the resulting feature maps is an item in the nth list of matrices of the dE\_dK. \bigskip

This is implemented in the code by first creating an empty dE\_dK list. Next, for an index in the range of the output depth, the dE\_dO matrix is gotten, at the index position in the dE\_dO. This is put into a list and the list is multiplied by the output depth, to get a list of size output depth where every item is the dE\_dO matrix. This list is zipped with the input from the input attribute, to pair up every input matrix with the dE\_dO matrix. A lambda function is mapped to all of these, which uses the mm.convolve funtion to find the convolution of every pair. This results in a list of feature maps. This is appended to the dE\_dK list. \bigskip

The dE\_dB is simply the dE\_dO. \bigskip

The dE\_dI can be found with the formula 
\begin{equation*}
    \frac{\partial E}{\partial X_{j}} = \sum_{i=1}^{n} \frac{\partial E}{\partial Y_{i}} \: \ast_{full}\: K_{ij}
\end{equation*}
However, implementing this formula in the code as it is was very difficult, so I derived my own method to implement the formula, using the transpose of the kernels matrix instead \bigskip

The transpose of the kernels matrix is found using the mm.transpose function., and create an empty list for the dE\_dI. Next, for an index in the range of the input depth, I found the list of matrices at the index position of the transposed kernels. I zipped this with the dE\_dO, and using a map and lambda function, aplied a \textit{full} convolution to all of the pairs in the zip list. The sum of all of these is found using mm.summatrix by looping through all matrices in the list, and finally the resulting matrix is appended onto the dE\_dO list. \bigskip

Finally, the biases are updated by subtracting the dE\_dB scaled by the rate. The same is done for the kernels using the dE\_dK, and the dE\_dI is returned.

\subsection{MaxPooling class}

The MaxPooling class is a class which inherits from the Layers class, used to implement a max-pooling layer. The layer takes in a 3D matrix, a list of 2D matrices. The purpose of the layer is to reduce the dimensionality of the matrices in the input, to reduce the amount of computations needed to handle it by the neural network. It does this by utlising the mm.pool function, selecting the 'max' option to perform a max pool.

\paragraph*{Pool function\\\\}

The pool function reduces the dimensionality of an image. It takes in the matrix as input, the size and stride of the kernel used in the pooling, and the mode of the pool. The mode can be max, min, or mean, to perform max-pooling, min-pooling and average-pooling respectively. \bigskip

The function begins by finding the dimensions of the input matrix with mm.dim, and creates an empty list to hold the output matrix. It also creates an empty list to hold the indices of the max 
pixels if the max-pool mode is selected, since these indices are needed by the max-pooling layer during back-propagation. \bigskip

The function essentially works by getting all of the positions where the top left corner of the kernel can be placed onto the matrix while fully contained by it, placing the kernel here, taking all of the values covered by the kernel, and using these to find a single value depending on the selected mode which becomes an entry in the output matrix. \bigskip

The first ((matrix number of rows) - (kernels number of rows) + 1) rows, at intervals equal to the stride, represent the rows at which the top left corner of the kernel can be placed. The same goes for the columns. Thus, the intersection of these rows and columns represent all the positions to place the top left corner of the kernel. The function uses a nested 'for' statement to access these positions. \bigskip

For a row index in range 0 to ((matrix number of rows) - (kernels number of rows) + 1), at steps equal to the stride, an empty row list is created, and another for loop is used to get the positions. For a column index in range 0 to ((matrix number of columns) - (kernels number of columns) + 1), at steps equal to the stride, an empty temp list is created and the kernel is placed with it's top corner at the position determined by the row index and column index. The values of the matrix that the kernel overlaps are placed into the temp matrix. \bigskip

What happens next depends on the mode. If the mode is 'max', then the maximum value of the temp list is taken and appended to the rows list. The index position of this max value from the matrix is determined, and added to the indices list. If the mode is 'min', then the minimum value of the temp list is taken and appended to the rows list. Otherwise, if the mode is 'mean', the average of the temp list is found and appended to the rows list. \bigskip

Once all the columns in the range are looped through, the rows list is appended to the output list. Finally, once all of the rows in the range are looped through, the output list is returned, along with the indices list.

\paragraph*{Specialised attributes\\\\}

The pool layer has attributes for kernel size and stride, which are values the layer is instantiated with. It also has an input size attribute, and a max position attribute, which contains the indices of all of the maximum items identified during the pooling process for each of the input matrices. Both of these attributes are set during the forward propagation, to be used in the back-propagation.

\paragraph*{Forward propagation\\\\}

The forward propagation simply applies the pool function to all matrices in the input, and returns the output. \bigskip

It begins by finding the dimensions of  an input matrix with mm.dim, and sets this value to the input size attribute. It also creates an empty list 'max\_pos' to store the list of indices of maximum items identified in the pooling for each of the input matrices. For each matrix in the input, the matrix is pooled with the mm.pool function, with the mode as 'max'. This returns the result of the pool and the list of indices of max item positions. The result is appended to the output list, and the max items indices list is appended to the max\_pos list. \bigskip

Finally, the max position attribute is set as the max\_pos list, to be used in back-propagation, and the output matrix is returned. 

\paragraph*{Backwards propagation\\\\}

The back-propagation finds the dE\_dI of the layer. In forwards propagation, the max items of the matrices in an area determined by the kernel size/stride is found, as well as the positions of these items, and the items are made into a new smaller matrix. The goal of backwards propagation is to turn the dE\_dO, which has the dimensions of the output matrix in forwards propagation, into the dE\_dI which has the dimensions of the input matrix in forwards propagation. In other words, where the forward pass decreases dimensionality, the backward pass restores this larger dimensionality. The existing items in the dE\_dO are all placed into the dE\_dI, at the maximum positions found in the forward propagation, and every other item in the bigger matrix is just 0. \bigskip

To implement this, the method begins by creating an empty list for the dE\_dI. For a index in the range of the number of the dE\_dO depth, a matrix of 0s is made, with the same dimensions as the input size attribute, using mm.makematrix. The list of max indices is gotten from the maximum positions attribute at the index of the loop. Next, the dE\_dO matrix at this index is gotten and flattened into a temp list. The nth item in the max indices list is the index corresponding to the position of the nth item of the temp list to be placed in the matrix of 0s. Thus, the first item in the temp list is placed into the matrix of 0s at the position of the indices from the first item in the max indices list, the second item of the temp list is places at the position of the indices of the second item in the max indices list, and so on, until all of the items in the temp list have been added to the matrix at their respective positions. \bigskip

This matrix is appended to the dE\_dI list, and the process repeats until it has been done for all matrices in the dE\_dO. Finally, the  the dE\_dI is returned.

\subsection{Test 3: MNIST convolutional}

After having created convolutional and pooling layers, I decided to test them on the same MNIST dataset as before, and compare the results to the neural network without the convolutional layer. \bigskip

The data used is the same processed data retrieved from a text file as used in Test 2. \bigskip

As with the previous test, creating a working neural network for this dataset took a great number of attempts, and also like the previous test, the solution created was a very simple and small one. Many CNN architectures for MNIST or other simple classification makes use of multiple convolutional and pooling layers, as well as many hidden layers. This would not work for me, as again my computational resources and time for training is limited, and so I would have to find something that would work for me balancing performance and training time. \bigskip

This was achieved by a neural network that has a convolutional layer that takes in a 28x28 matrix of the image and applies a convolution with a 5x5 kernels to the image to turn it into 3 images all 24x24 in size. This is passed through a ReLU. The result is passed into a max-pooling layer of a 2x2 kernel with strides of 2, effectively halving the dimensions of matrices, so that the data goes from 3x24x24 to 3x12x12. Multiplying 3 by 12 and 12 gives 432, so the next layer is a hidden layer of 432 neurons. This is followed by a ReLU layer, then a softmax, and finally a hidden layer of 10 neurons to output the classification. \bigskip

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=18cm]{Images/MNISTCONV.drawio.png}}%
\end{figure}
\bigskip

This was trained with a batch size of 1 and learning rate of 0.1, over 10 epochs with 1024 of the images from the dataset used. Despite the neural network appearing small and simple, and using a tiny fraction of the whole training data to train on, in testing it yielded an astounding percentage accuracy of 95.06\%.

\subsection{Image augmentation}


With the neural network and all of it's layers created, it is time to begin training the neural networks for their purpose in the program. \bigskip

However, to train the neural network to recognise the element number of LEGO pieces by their shape from an image, it is not enough to just train it on the raw training data of computer rendered images and then expect it to work for images from the real world. The images of the dataset all share common characteristics, such as the colours, position of the LEGO item in the image, the size of the item, texture, etc. The neural network may be fitting itself to these properties. In images of real LEGO pieces, the size, position, sharpness, brightness and whatnot will all be different in every image, so passing these into the neural network trained on the raw dataset will yield poor results. \bigskip

Thus, to prevent the neural network from fitting to these characteristics instead of the shape of the LEGO item as it is expected to, it is important that the training data varies these characteristics in it's images. This, described in the research section of the analysis, is known as domain randomisation, and involves randomly tweaking the different aspects of the image to prevent the machine learning model from over-fitting to them. \bigskip

To achieve this, I created an Augmentation class. The class contains static methods that allow it to take an image and tweak one of these characteristics of the image, such as the size of the LEGO item in the image or brightness of the image. It also contains a static method 'augment' which takes an image and applies randomly some of these augmentations, by random amounts of extremeness, and returns the new randomly augmented image. This is used in every item of the dataset, and ensures that every image varies in characteristics to be more like the images that will actually be passed into the neural network by the user, improving performance. \bigskip

The Augmentation class utilises the PIL library to perform changes to the image. The static methods of the class are represented in the class definition below. \bigskip

\begin{table}[H]
\begin{tabular}{|l|l|l|}
\hline
Method     & Input/Output                                                                                & Description                                                                                                                                                                                                                                                                                                                                                                       \\ \hline
augment    & \begin{tabular}[c]{@{}l@{}}Input: image, invert\\ = False\\ Output: new image\end{tabular} & \begin{tabular}[c]{@{}l@{}}Applies a random geometric augmentation from 3 options:\\ rot, crop, or zoom. Next, it applies a random colour augme-\\ ntation from 2 options: contrast or brightness. It then\\ applies a sharpness augmentation, and finally if invert is\\ True, then applies an invert augmentation on the image.\\ Returns this new augmented image\end{tabular} \\ \hline
rot        & \begin{tabular}[c]{@{}l@{}}Input: image\\ Output: new image\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Returns the image randomly rotated between 0 and 360\\ degrees\end{tabular}                                                                                                                                                                                                                                                                            \\ \hline
crop       & \begin{tabular}[c]{@{}l@{}}Input: image\\ Output: new image\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Crops the image such that the position of the item in the \\ LEGO is off-centred, randomly. Returns new image\end{tabular}                                                                                                                                                                                                                             \\ \hline
zoom       & \begin{tabular}[c]{@{}l@{}}Input: image\\ Output: new image\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Zooms into the LEGO by a random amount, effectively\\ resizing the item in the image randomly. Returns this new\\ image\end{tabular}                                                                                                                                                                                                                   \\ \hline
contrast   & \begin{tabular}[c]{@{}l@{}}Input: image\\ Output: new image\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Returns the image with the contrast changed by a random\\ amount\end{tabular}                                                                                                                                                                                                                                                                          \\ \hline
brightness & \begin{tabular}[c]{@{}l@{}}Input: image\\ Output: new image\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Returns the image with the brightness changed by a random\\ amount\end{tabular}                                                                                                                                                                                                                                                                        \\ \hline
sharpness  & \begin{tabular}[c]{@{}l@{}}Input: image\\ Output: new image\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Returns the image with the sharpness changed by a random\\ amount\end{tabular}                                                                                                                                                                                                                                                                         \\ \hline
invert     & \begin{tabular}[c]{@{}l@{}}Input: image\\ Output: new image\end{tabular}                   & Returns the image with the colours of the image inverted                                                                                                                                                                                                                                                                                                                          \\ \hline
aspect     & \begin{tabular}[c]{@{}l@{}}Input: image, mode\\ = 'Min'\\ Output: new image\end{tabular}   & \begin{tabular}[c]{@{}l@{}}Resizes the image to be of 1:1 aspect ratio, by cropping the\\ image to be either smaller or larger\end{tabular}                                                                                                                                                                                                                                       \\ \hline
\end{tabular}
\end{table}
\bigskip

The crop and zoom augmentations assume that the LEGO item is centred and makes up roughly 60\% of the width and height of the image, as is the case with the LEGO in my dataset. Thus, it applies it's transformations in such a way to ensure that the whole of the LEGO item is still contained in the image. \bigskip

The aspect method is not included in the augment method, but is still used multiple times throughout the program. It takes in the image as well as a mode, defaulted at min. If mode is min, then the image is resized to 1:1 by the smaller side of the image. The image is cropped to be smaller, to be of dimensions smaller side x smaller side. On the other hand, if the option is set to max, then the image is cropped to be larger, to be of dimensions larger side x larger side. This adds black edges to the image to make the size of the sides of the image equal.

\subsection{Colour neural network}

The colour neural network is used in the main program by the getColour function. The neural network is fed a normalised RGB value, and outputs a matrix of predictions used in conjunction with the colour list to classify the LEGO colour from the RGB value, from 14 different colours. \bigskip

As mentioned in the proposed section, this utilises a dataset created by the 'ICT institute' which contains RGB values taken from images of LEGO pieces, and the LEGO colour of this piece. These are stored in a .csv file, and I created a colour\_proc function to process the data in this file and return lists for the data and results (labels of the data), as well as a colour list which contains all of the colours, in the order that the neural network will output its predictions in the output matrix. This is the same colour list as is used in the main program, and is used in conjunction with the output matrix of the colour neural network to get the classification. \bigskip

The neural network consists of a hidden layer of 3 neurons, to take the three values of RGB as input, a ReLU layer, a hidden layer of 100 neurons, a ReLU layer, a softmax layer and a hidden layer of 14 neurons for the predictions of the 14 colours. The neural network is trained on the dataset for 50 epochs at a learning rate of 0.1 and batch size of 1. The accuracy of the neural network is FIJFUAFBAEVFA VFAGFAGFAHGF AF ASFSGF:ESAGF"OAGFOSAUEF which I am happy with. \bigskip

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=17cm]{Images/COLOURNN.drawio.png}}%
\end{figure}

\subsection{Element neural network}

The element neural network is used in the main program by the getElem function. The neural network is fed the image after it undergoes processing and normalisation as input, and it outputs a matrix of predictions. This is used in conjunction with an element list to classify the LEGO element number of the LEGO item in the image, from 10 different values. \bigskip

The data used to train this neural network comes from a Kaggle dataset as specified in the proposed solution. The dataset contains 40,000 images of 50 different LEGO pieces, rendered by a computer. Training a neural network on all of this data would require a large complicated architecture, and would be too much for me to pull off with my resources and time. Therefore, I selected 10 LEGO items of these 50 to base my neural network off of. These were selected based on the most common LEGO elements, as well as being quite different from each other, since a lot of elements look the same from certain angles, which would make recognition harder for the neural network. For all of these 10 elements, I hand selected 350 of the best images of the element from the dataset. Some items are extremely difficult to identify, even for a human, from certain angles, so these were excluded. Furthermore, it is unlikely for an image to be of a LEGO item upside down, and the user is prompted to use nice angles anyways, so I only picked these nice items for the neural network to make it's training easier. \bigskip

The images are stored in a file which is read from by the procLego function. The function has empty lists for the training data, training results, testing data and testing results. It also has a count, set at 0. The images in the file are all ordered already by their element. For every image in the file, the image is opened using PIL and augmented to introduce domain randomisation using the augment function. The invert is kept at False, since inverting the image completely threw off the neural network training and resulted in a neural network that learnt terribly slow. Thus, instead of getting the neural network to work with images of white background, I decided the best roundabout solution would be to process any images input to the neural network by the user to have a black background, as is explained in the getElem paragraph of the subroutines section. Once the image from the dataset is augmented, it is then resized to be of size 64x64. \bigskip

The label for the data is then created by creating a matrix of 10 items set at 0, and making one of these items 1 at the position that represents the element. The image is turned into data, normalised by dividing all items by 255 so that every item is between 0 and 1, and the resulting data is made into a matrix. If the count is more than or equal to 300, the image matrix is added to the testing data and the label matrix to the testing results, otherwise the image matrix is added to the training data and the label matrix to the training results. If the count is equal to 350, it is reset to 0. Since the LEGO are already ordered by element, this process separates the 350 items of an element such that 300 of the items are trained with and 50 of the items are tested with. The function returns the training data, training results, testing data and testing results. \bigskip

The training data/results are used by the neural network to train. Despite reducing the complexity of the data by handpicking elements and their images, the domain randomisation of the images increases the complexity of the data by a significant amount. However, the domain randomisation is necessary for the neural network to function on actual images of LEGO. Thus, since the task is complex, a more complex neural network is needed than was used with the MNIST data. Both MNIST neural networks taught me to not waste days at a time trying to train a complex neural network and instead stick to more simple architectures, which can yield decent result for a fraction of the time and resources. This was harder to achieve with the element neural network given how much more complicated the task is than just hand-written digits. \bigskip

The neural network consists of a convolutional layer which takes in a single matrix that is 64x64, and returns 8 matrices that are 62x62 by performing a convolution on the input matrix with kernels of size 3x3. This layer is followed by a ReLU activation layer, and then a max-pooling layer of kernel size 2x2 and stride 2 to effectively halve the dimensions of the input matrices, turning it from 8x62x62 to 8x31x31. The results of 8 multiplied by 31 and 31 is 7688, so the next layer is a hidden layer of 7688 neurons, followed by a ReLU layer and then a softmax layer. Finally, a hidden layer of 10 neurons outputs the matrix of predictions. \bigskip

\begin{figure}[H]
  \makebox[\textwidth][c]{\includegraphics[width=19cm]{Images/ELEMNN.drawio.png}}%
\end{figure}
\bigskip

The procLego function is called 4 times to combine 4 sets of training and results data. Despite using the same images from the dataset, the data returned by each of these calls is different because of the domain randomisation, which randomly changes each image. This effectively allows for the neural network to be trained on 12000 different images instead of 3000. A batch size of 1 is used, with a learning rate of 1 to allow for fast convergence, over 10 epochs. This training takes a couple of days, and brings the percentage accuracy to about 50\%. The percentage accuracy is found by testing the neural network on the 4 sets of testing data, which is 2000 images. \bigskip

The resulting neural network is then trained further, with a batch size of 1, but this time a learning rate of 0.1 to fine tune the learning at this stage a bit more, over 5 epochs. This brings the percentage accuracy up to J UAEGLAEAGVEFGA, which is not as large as I had hoped but probably expected given the complexity of the task- especially given my time and resources limits. Trying to find an optimal architecture/values for the hyper-parameters took over a month of training, over many different attempts, so I can settle with the final percentage accuracy. \bigskip

Given more time to experiment and train however, this percentage can most definitely be boosted.


\end{document} 
